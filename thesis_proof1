%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
% DOCUMENT CLASS
\documentclass[oneside,12pt]{Classes/RoboticsLaTeX}

% USEFUL PACKAGES
% Commonly-used packages are included by default.
% Refer to section "Book - Useful packages" in the class file "Classes/RoboticsLaTeX.cls" for the complete list.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{epigraph}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{float}
\usepackage{longtable}
\usepackage[pdftex]{graphicx}
\usepackage{pdfpages}
%\usepackage{tabularx}
\usepackage{pdflscape}
\usepackage[acronym,toc]{glossaries}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\setstretch{1.5}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[section]{placeins}


%\onehalfspacing
% SPECIAL COMMANDS
% correct bad hyphenation
\hyphenation{op-tical net-works semi-conduc-tor}
\hyphenation{par-ti-cu-lar mo-du-le ge-stu-re}
% INTERLINEA 1.5
%\renewcommand{\baselinestretch}{1.5}

%% ignore slightly overfull and underfull boxes
%\hbadness=10000
%\hfuzz=50pt
% declare commonly used operators
%\DeclareMathOperator*{\argmax}{argmax}

% HEADER
\title{\Large{Evaluation of a Reinforcement Learning Approach to Virtual Machine Selection}}

  \author{Luke Hayes - 14498098}
  \collegeordept{School of Computer Science}
  \university{National University of Ireland, Galway}
  \crest{\includegraphics[width=120mm]{Figures/nuig_logo.jpg}}


\supervisor{Dr. Enda Howley}
%\supervisor{Name of the Supervisor}
%\supervisor{Name of the Co-Supervisor}	
% \supervisor{Dr. Jane Smith}
% \supervisorSecond{Dr. Mihael Arcan}
                                                                                                                 
% text before "In partial fulfillment of the requirements for the degree of" in .cls file/line 153\
% replace PROGRAMME with Data Analytics, Artificial Intelligence, or Artificial Intelligence - Online
\degree{MSc in Computer Science (Artificial Intelligence)}
\degreedate{June 29, 2021}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% uncomment if glossary needed, see examples in file
%\makeglossaries
%\loadglsentries{glossary}

\begin{document}
\begin{spacing}{1}
\maketitle
\end{spacing}

% add an empty page after title page
\newpage\null\thispagestyle{empty}\newpage

% set the number of sectioning levels that get number and appear in the contents
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}


\frontmatter
% replace PROGRAMME with Data Analytics, Artificial Intelligence, or Artificial Intelligence - Online
\textbf{DECLARATION} 
I, NAME, do hereby declare that this thesis entitled Machine Learning Approach to Data Centre Energy Optimization is a bonafide record of research work done by me for the award of MSc in Computer Science (Artificial Intelligence) from National University of Ireland, Galway. It has not been previously submitted, in part or whole, to any university or institution for any degree, diploma, or other qualification. 
\newline

\begin{tabular}{@{}p{.5in}p{4in}@{}}
Signature: & ~~\hrulefill \\
\end{tabular}
\newpage
\mainmatter

\begin{abstracts}
An increase in commercial usage of services that utilize cloud computing has led to the rapid growth of the required resources to facilitate this environment. Included in these resources of ever-increasing demand are extremely large data centers that utilize vast amounts of power and energy. This large use of energy coupled with poor efficiency of the vast majority of data centers is a large issue of concern. The research carried out in this thesis, aims to evaluate an approach to dealing with data center inefficiency, thus reducing energy consumption in the process. A Reinforcement Learning approach to Virtual Machine (VM) Selection will be both developed and evaluated. VM Selection is the process of selecting a VM to be migrated to another host in the case where a host has become over-utilized. Intelligent selection of VMs to be moved to another server, will lead to less hosts becoming both over and under utilized, resulting in energy savings. The proposed algorithm achieves a daily saving of 10.37\% in energy consumption when compared to a state-of-the-art algorithm, Lr-Mmt. This thesis confirms that a Reinforcement Learning approach to VM Selection is a viable approach with scope for improvement.

\end{abstracts}

\tableofcontents
\listoffigures
\listoftables
\printglossary[title=List of Acronyms,type=\acronymtype]

\chapter{Introduction}
\label{chap:introduction}
Cloud computing has quickly become one of the most influential technologies available to modern businesses. Cloud computing provides flexible and secure on-demand availability of computer system resources. Businesses no longer require local servers that are both fixed in bandwidth and storage. Companies often spend a large amount of time and resources dealing with such issues arising from local servers. Some of the many advantages of adopting a cloud computing framework include flexibility, scalability and security. These improvements to business operation have led to the adoption of the cloud computing model across many industries and sectors. Therefore, leading to a surge in demand and development of data centers. These data centres are buildings with a huge number of interconnected computer servers. Many people are unaware that a task as simple as watching a movie on Netflix will require use of resources located in a data center somewhere across the world. 

This growth in demand has however, come at a major cost of a sharp increase in energy consumption, resulting in increased carbon emissions. It was in fact estimated that the worldwide carbon footprint of the data centre industry would eclipse that of the airline industry in 2020 [1]. This is a greater problem in the country of Ireland where data centres consumed 11\% of the Irish energy bill for 2020 [2]. In 2006 data centres were estimated to have accounted for 1.5\% of the total U.S. electricity bill [3]. While both these statistics are taken from quite different time frames, we can conclude that Ireland is considered a data center hot spot. A study conducted by EirGrid, concluded that by 2028 data centres could account for 29\% of Ireland's energy bill [4]. It also concluded that the energy costs of Irish data centres will double every 5 years. This increase in energy consumption coupled with the universal issue of global warming presents perhaps one of the biggest challenges that the Information Technology (IT) industry currently faces. 

One approach that has been developed that facilitates a reduction in data centre energy costs is the concept of virtualization. This concept enables cloud computing the achieve its dynamic and scalable nature. It does so by splitting the resources of the server into much smaller independent Virtual Machines (VMs) which will run on their own host. Each VM on the server appears as if it is running in complete isolation and runs with its own Operating System (OS), which allows for multiple tasks to be run in parallel to one another. This therefore allows increased server utilization which means that less servers will be required, allowing for savings in energy consumption. The separate OS's will also lead to better resource utilization further reducing the energy consumed. While Virtualization does lead to an increase in energy efficiency data centres are still hugely inefficient due to huge under utilization of resources the majority of the time. In fact statistics gathered by the NRDC state that the average data centre operates between 12-18\% of its maximum possible capacity [5]. This statistic highlights the requirement of improved policies to increase server and resource utilization to reduce data centre energy consumption.

When looking to increase data center efficiency one must also consider the Service Level Agreements (SLA) that each data centre will be required to adhere to. This concept adds greater difficulty as now the service being supplied by the data centre must reach and maintain a specified level of quality. To balance the conflicting goals of energy optimization and adhering to SLAs, concepts such as VM Selection and VM Placement are introduced. VM Selection required selection of VMs to be moved from hosts that have become over-utilized. This enables SLAs to be adhered to while also optimizing energy consumption. Choosing the right VM to move will allow the host to no longer be over-utilized. VM Placement focuses on where this selected VM is then placed. Choosing the correct new host for the VM will enable hosts to be utilized effectively meaning less hosts will be required to be active. Both VM Selection and Placement are very effective policies for ensuring SLAs are adhered to while also optimizing energy consumption. However, these are computationally expensive. VM Placement is an NP-hard problem, with the computation cost growing exponentially with each new server added. This issue coupled with the extremely dynamic workload of each VM, means these policies are highly difficult to implement. 

The aim of this thesis is to evaluate an alternative approach to solving the problem of VM Selection. The approach taken is a machine learning approach called Reinforcement Learning (RL). This application is much more dynamic and will be carried out continuously, such that it can deal with the constantly changing workload. Reinforcement Learning utilizes an autonomous learning agent that will be motivated through the idea of a reward. This reward will incentivize the agent to keep energy costs to a minimum while also obeying the imposed SLAs. Goals will enable the agent to observing the environment that is the data centre, and then taking actions to maximise those goals. These actions will consist of a VM Selection action, meaning the agent will select a VM to move from the over-utilized server. Through clever VM Selection, the energy consumption of the data centre can be reduced, while also providing the satisfactory customer experience. 


\section{Problem Formulation}
The goal of this research is to explore the possibility of utilizing Reinforcement Learning (RL) to optimize the energy utilization of data centres. As explained in the Introduction, data centres are inefficient due to underutilized servers meaning more are running than required. This research aims to implement an effective VM Selection policy that in conjunction with a VM Placement policy, will optimize the allocation of VMs across the data centers servers. This optimized allocation will ensure less servers are active, reducing energy consumption. The objective of this research is to go beyond that of the current state-of-the-art.

Figure 1.1 illustrates the motivation behind this research. In this graphic there are four servers with three VMs in each. The image has an unoptimized data center (left) and an optimized data center (right). There is significantly less wasted resources on the optimized side and there is also now two servers that are no longer being utilized. Both of these improvements lead to less energy being consumer by the data centre. Studies have shown that servers that are operating at low capacity can still utilize up to 60\% of the servers potential maximum power [6]. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Figures/Optimized_Vs_Standard_DC1.PNG}
\caption{Data Centre - Standard Approach Vs. Optimized Approach}
\end{figure}

Another factor to consider is the requirement to obey the Service Level Agreements (SLAs) that have been put in place. These SLAs determine the level of service that must be provided to ensure that the customer receives the expected Quality of Service (QOS). Therefore, a balance is required between these two conflicting goals. Reducing data center energy consumption will likely lead to more SLA violations. This is therefore a trade-off which must be adequately balanced. 

\section{Thesis Research Questions}
This research aims to answer each of the following questions:

\begin{enumerate}
\item When utilizing the Q-Learning algorithm for VM Selection, what parameters of Alpha and Gamma facilitate the best performance?
\item When utilizing the SARSA algorithm for VM Selection, what parameters of Alpha and Gamma facilitate the best performance?
\item When utilizing a Reinforcement Learning Approach to VM Selection, is Q-Learning or SARSA the better performing algorithm?
\item How does the performance of Reinforcement Learning (RL) approach presented compare to Lr-Mmt, a state-of-the-art VM Selection policy?
\end{enumerate}


\chapter{Background}
\label{chap:backg}

\section{Overview}
This chapter aims to provide a detailed understanding of each technology that is utilized within this research. Firstly, the different type of computing will be explained. This will lead to a comprehensive look at cloud computing including the characteristics, architecture and different application types. Next an analysis of how data centers consume energy will be carried out. The technology of virtualization will be introduced next. Finally, the area of Machine Learning (ML) and its subtype Reinforcement Learning (RL) will be explored. 

\section{Cluster Computing}
This concept involves several computers that are connected via a network that operate together as a single entity. This idea was generated in the 1980's when supercomputers were at the forefront of High-Performance Computing (HPC). These supercomputers led to the realisation that computers could carry out these tasks, that required huge computational power, without the excessive costs of supercomputers. This cluster of distributed computers are connected via a high speed connection such as Local Area Network (LAN). These connected computers are capable of splitting the main task into sub tasks carried out in parallel, at a fraction of the time.

The cluster computing approach gives high availability and is also fault tolerant. This is due to the fact that multiple nodes will often run the same job. This means that should one node be lost, the other node will carry out the task. The framework also promotes load balancing as a load balancer can be implemented to the network to ensure that no node gets a disproportionate amount of work. Health checks can also be carried out to ensure all nodes are still up and running. These characteristics are some of the main reasons that the cluster computing framework is utilized in storage and backup facilities for major companies such as Google [7].

\section{Grid Computing}
Grid computing, like cluster computing is an environment for large-scale applications. However, unlike cluster computing which leverages a cluster of computers with the same capabilities, grid computing leverages the computational resources of a large number of dissimilar devices [8]. Each node in the grid is set such that it will complete a different task/application. As with cluster computing all nodes are connected via a network and they are working together to achieve a common goal. Grid computing was designed to carry out tasks that requires more computational power than the existing cluster and supercomputer frameworks [9]. The idea here is that the resources available can come from multiple different domains. Each grid is made up of several Virtual Organizations (VO's). Each VO brings resources and in return receives access to a much wider pool of resources that it would not have had on its own. This means that there is essentially a community of users that can utilize the resources to achieve their own defined goals [8]. Such an approach is still widely adopted in many academic research initiatives, in departments such as engineering. Grids are fault tolerant as many different nodes can carry out the same task should there be a failure. This framework also makes much better use of idle resources. However, it is worth noting that security is a large flaw of this computing framework.  

\section{Cloud Computing}
Cloud computing capitalises on many of the technological advancements of previous computing models. It has now become the most prevalent computing architecture and has been adopted as the gold standard approach in many domains. This framework means that companies no longer need to spend money to purchase and maintain their own server hardware. Previously, this would have required hiring skilled employee's to look after such tasks. This approach is also highly dynamic as the resources required can scale with both demand and the growth of the business. Many providers model their cloud computing products as a "pay-as-you-go" model. This is highly appealing for companies. The framework is also both reliable and secure, of which the latter was a concern in the grid computing framework.

In 2006 today's biggest enterprise in the space of cloud computing, Amazon Web Services (AWS), launched Amazon Elastic Compute Cloud. This was a web service that provided scalable computing capacity. It was specifically aimed at web developers and essentially enabled computing in the cloud as it allowed for the renting of virtual computers. It wasn't until the year 2008 until other companies such as Google and IBM began to come into the frame. This gap in time could be seen as one of the reasons that AWS is the current market leader in the area of cloud computing. 

While Cloud Computing does utilize technologies previously developed, it has unique characteristics that have led to the need for development/enhancement of technologies in the areas of data centers and Virtualization. Cloud computing has propelled the interest in these areas and thus has led to technological advances that have paved the way for cloud computing to be the dominant computing model of today. 

The definition of cloud computing is something that has not yet been agreed upon. Therefore, there are many definitions that are deemed acceptable. Vaquero et al. conducted research in an attempt to select a suitable definition of Cloud Computing [10]. This paper included research into 20 potential definitions which highlights the difficulty in selecting a definition the industry will agree upon. While there are many definitions there are some that are more widely accepted such as the following. 

Buyya et al. gave the following definition [11]: "A Cloud is a type of parallel and distributed system consisting of a collection of interconnected and virtualized computers that are dynamically provisioned and presented as one or more unified computing resources based on service-level agreements established through negotiation between the service provider and consumers"

In September 2011 the National Institute of Standards and Technology, a government agency that aims to promote competition and innovation amongst U.S. based tech companies in both technology and science published a paper with their own definition [10]. This following is the definition they concluded their research with: "Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction".

While there are many definitions, Vaquero et al. looked at the terms that were important in the definition by looking at the reoccurring terms in each of the 20 definitions. They concluded that the most important characteristics of cloud computing motioned in the definition were that cloud computing was scalable, the payment model was "pay-per-use" and that it relied heavily on technologies such as the internet and virtualization.

\section{Cloud Computing Characteristics}
This section will look at some of the essential characteristics of Cloud Computing and will highlight the characteristics that have led to this technology becoming so widely used. The characteristics outlined in this chapter will be those that differentiate cloud computing from the other computing architectures available. 

\subsection{Cost Savings}
A reduction of costs is one of the main reasons why businesses choose to implement cloud computing within their business [12]. One of the main ways Cloud Computing enables financial savings is the reduction of upfront investments. This investment is typically spent on the infrastructure and software required to host many of their resources. This cost is replaced by the "pay-per-use" payment model implemented by the Cloud Computing framework. This means essentially that processors are charged by hourly use and storage is charged by daily storage. This model of only paying for what is used saves a great amount of money for the business. Were the company to to have their own computing resources located on-site, they would have to also train the IT support staff to be able to deal with issues. They may also have to hire additional staff to deal with the increased burden on their IT support staff [14]. Finally, the companies that provide cloud computing services are operating at a huge scale and therefore benefit greatly from economies of scale and thus this reduces the costs even further [15]. 

\subsection{Resources are Supplied On-Demand}
One of the main issues with computational resources is that the demand can vary dramatically through the day. One example of this would be Netflix, here demand would be very low during off peak hours, where people are working, and very high in the peak hours. If a company were to have their own resources they may not be able to predict the resources that will be required on peak and therefore may not reach their Quality of Service agreements with the customer. Moreover, there will be a large waste in resources on the off peak hours [15]. Cloud computing solves this problem as there is no limit on the computing resources that are available. When the required computing resources, storage or services increase, so too does the resources provided from the cloud. To the consumer it seems as if there is unlimited service and storage capacity [11]. 

\subsection{Access}
Another big advantage is the flexibility of access provided as now resources can be accessed very easily by employee. This access can be gained by utilizing network protocols such as Transmission Control Protocol/Internet Protocol (TCP/IP) or Hyper Text Transfer Protocol (HTTP). This facilitates access via multiple different devices [16]. One huge advantage of this is that it enables remote working which is something that has been forced on businesses worldwide in consequence of the global Covid-19 pandemic. This characteristic has meant that cloud computing has become almost essential for business operation in the current climate. 

\subsection{Agility}
A final advantage is that it allows businesses to be agile and flexible in a business world that is dynamic and constantly changing. Today it is often the company that is best placed to adapt that will succeed. Companies that utilize a cloud computing paradigm are enabled to implement these changes as fast as possible [17]. Changes that would typically take days if a company were to have their own server can take minutes in a cloud computing environment. The internet plays a big role in the fact that cloud computing can enable changes in a very short time making it an effective tool for rapid development [18].    

\section{Cloud Computing Architecture}
Cloud computing provides enterprises with a unique approach to managing their IT resources. Cloud computing providers essentially have created a market for IT services. There are however many different services that can be provided via cloud computing. These services are made up of three different types. Software as a Service (SaaS), Platform as a Service (Paas) and Infrastructure as a Service (IaaS). These three services are known as the service models and a company may require one or more of these from the cloud computing provider. 

The cloud computing architecture is made up of four layers, each of which is dependent on the other. As shown in figure 2.7.1 below, these layers are the Application layer, Platform layer, Infrastructure layer and the Hardware layer. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Figures/Cloud_Architecture.PNG}
\caption{Cloud Computing Architecture [14]}
\end{figure}

\subsection{Application Layer}
This is at the top layer of the architecture and consists of the applications which support the SaaS approach. This is a service that can be accessed by the general public via the internet. Customers are generally charged on subscription basis and typically users will pay monthly with the option to cancel at any moment [15]. Users can begin availing of this type of service immediately once they sign up. Many companies will utilise this SaaS approach to giving their services such as YouTube. Cloud applications can utilize all of the features associated with cloud computing such as the ability to automatically scale the required resources.

\subsection{Platform Layer}
This layer is built on top of the infrastructure layer and is the PaaS service model. This layer is built up of Operating Systems and application frameworks. The services in this layer give developers the tools to create and deploy cloud applications. The main advantages of this approach is that it facilitates scalability and load balancing of new applications [20]. Another advantage to the user is that the supplier will manage all of the tools required to build the applications such as libraries, frameworks and the infrastructure (e.g. storage). Today many teams utilize an agile approach to development and the PaaS model works very well with this approach as it promotes very fast development [21].

\subsection{Infrastructure Layer}
This layer can also be known as the virtualization layer [13]. Through virtualization to create VMs, this layer creates numerous computing resources from the physical resources (e.g. servers). This is a critical layer in cloud computing as it facilitates dynamic resource assignment which is a key characteristic of the cloud computing paradigm. This layer leverages the IaaS service model as businesses can hire dynamic cloud infrastructure. One of the main features of this layer is that customers have super-user access to their VM's and thus have full control over the software stack utilized in their VM's. 

\subsection{Hardware Layer}
This is the layer that contains the physical resources that are required within the cloud. This would include the routers, servers, power equipment and the cooling equipment to keep equipment from overheating [19]. All of this equipment is located in the data centre in cloud computing. A data centre is typically made up of thousands of servers that are connected via routers. This layer can also be provided as a service named Hardware as a Service. This is where a data centre is rented out by an enterprise. This is highly expensive and thus is far less common than the three services mentioned earlier. There are many issues that must be managed at this layer such as fault tolerance, management of traffic, hardware configuration and management of power and cooling resources [19].


\section{Types of Clouds}
There are a number of items to consider when a company is choosing to migrate an application to a cloud computing environment. There are four types of cloud environments to choose from, each of which having unique strengths and weaknesses. The team of the company will need to access the best cloud environment for the application they are migrating. The four cloud types are private, public, hybrid and community. 

\subsection{Public Cloud}
This is an environment where the computing resources are offered to the general public. This cloud environment will typically operate on a pay-per-use payment model. This type of model offers many of the key benefits to cloud computing such as cost reduction through not having to invest in cloud computing infrastructure, while also allowing the company to remain agile. It also shifts many of the risks of traditional computing to the provider such as downtime (a time where cloud services are unavailable). One disadvantage however, is the lack of control over the data, network and security settings [19]. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.3\textheight]{Figures/Public_Cloud.PNG}
\caption{Public Cloud Model [22].}
\end{figure}


\subsection{Private Cloud}
Private clouds or internal clouds are utilised by one organization, they are not open to the public as with public clouds. There are two options here, either the cloud is built and maintained by the organization or by an external party. Either way the cloud is only made available to that organization. The main advantage of such an approach is that it gives the greatest level of control over security, performance and reliability [13]. The main drawback however, is the cost as the company must now pay a large up-front costs to setup the data centre. Therefore, such an approach typically can only be afforded by very large businesses. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.3\textheight]{Figures/Private_Cloud.PNG}
\caption{Private Cloud Model [22].}
\end{figure}

\subsection{Hybrid Cloud}
This is a combination of both the public and private cloud models. The reason for its inception was to address the limitations of both models and thus create a better approach. In this framework both models remain unique entities but they are connected together via approaches and technologies that allow for easy moving of data and applications between both entities. The main advantage given by this architecture is that it gives better control and security of application data, which was a flaw with the public cloud model. However, one negative is that it requires careful splitting of the resources between both the public and private entities. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.3\textheight]{Figures/Hybrid_Cloud.PNG}
\caption{Hybrid Cloud Model [24].}
\end{figure}

\subsection{Community Cloud}
The final approach to Cloud Computing is the Community Cloud. Here the cloud is setup by a set of organizations that have a common goal or interest. One example of a group of organizations that may create a community cloud is a series of banks. Here each individual company has similar requirements in areas such as security and privacy. This cloud can be managed by a provider or in-house by one of the organizations [25]. The group will also likely create a mechanism by which they review the new companies looking to gain access. 

\begin{figure}[h]
\centering
\includegraphics[width=.7\textwidth, height=.3\textheight]{Figures/Community_Cloud.PNG}
\caption{Community Cloud Model [26].}
\end{figure}

\section{Energy Consumption of Data Centres}
Data centres are becoming more and more prevalent due to the meteoric rise in demand for the Cloud Computing paradigm. This increase in demand has become even more relevant since the beginning of the pandemic as businesses aim to transition to enable employees to work from home. The pandemic has led to an increase in demand for Cloud Computing from the education, entertainment and business industries [27]. This rise in demand saw huge disruptions to services such as Microsoft Teams as a huge number of new users were created as remote working was forced upon businesses. Netflix were forced to reduce the resolution of their streams to cope with this huge increase in demand [28]. This has resulted in increased demand which has increased energy expenditure from the data centre industry. It is estimated that by 2027, 37\% of Ireland's energy will be consumed by data centres [29]. 

It is however worth noting that despite the increase in data centres workloads, the energy consumption of data centres worldwide has remained at around 1\% [30]. The reason for this is rapid increases in data centre energy efficiency. This is exceptionally impressive considering data centre computation has increased 550\% between 2010 and 2018 [31]. However, a deeper analysis reveals that there are a small number of companies making huge strides in this space. These efficient data centres belong to giant multinational companies such as Google, Facebook and Amazon whom only posses between 5 and 7 percent of the data centres in the world [32]. With the majority of inefficient companies simply leasing their cloud computing resources rather than creating their own data centres, there is little incentive to increase efficiency. The companies that are highly efficient are industry tycoons, therefore they are placed under worldwide scrutiny. They aim to reduce energy consumption in an attempt to reduce public and governmental scrutiny. 

Data centre energy efficiency is often measured in the literature by a metric called Power Usage Effectiveness or PUE. This metric essentially is a ratio of total data centre power to the power utilized by the IT equipment [35]. Highly efficient data centres would achieve a value of around 1.2 or less while inefficient data centres would receive a PUE score of 2.0. This score of 2.0 means that for every unit of energy utilized by the IT infrastructure there is another unit used elsewhere. In 2017, a company named Supermicro achieved a PUE of 1.06.  


\subsection{Breakdown of Data Centre Energy Consumption}
As outlined in the previous section the vast majority of data centres are highly inefficient and therefore, utilize a large amount of energy. For improvements to be made, it is critical to understand what are the elements of the data centre that are contributing the most to this inefficiency. This is crucial so that technologies can be developed/implemented such that there is a reduction in energy usage. 

Figure 2.6, from a 2016 study carried out by Rong et al., breaks down the different areas of data centre energy utilization by percentage [33]. From this figure it is evident that the majority of energy is consumed by the cooling system and the servers. Together both of these areas take up an estimated 80\% of the energy utilized by the data centre. 40\% of the total energy being used to power the servers, provides great motivation for this research. While huge energy savings have been made in the areas of hardware and equipment, there have been far less improvements in the area of server utilization [32]. Servers are often left underutilized which is a major problem considering that idle servers still can draw 70\% of the servers maximum power. This is compounded by the fact that studies have show that between 20-30\% of data centre servers are idle [34]. With all these statistics it is clear that server power consumption provides huge scope for improvement. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.35\textheight]{Figures/Data_Centre_Energy_Breakdown4.PNG}
\caption{Breakdown of Data Centre Energy Consumption [33]}
\end{figure}


\section{Virtualization}
Virtualization is a technology that is fundamental to both the development and the adoption of the cloud computing paradigm. It enables the users of applications and services to ignore the computing resources that would otherwise be finite in their nature [36]. The main core idea here is that it creates a virtual version of something such as a server, OS, computer hardware resources or storage devices such that these resources can be utilized on several different machines simultaneously. Virtualization has transformed tradition computing by allowing it to become more scalable and economical as it reduces both cost and power. It essentially enables servers to be split into multiple VMss, each of which has their own OS and can run independently. It is a fundamental concept that allows cloud computing to boast its characteristics of having both highly scalable and elastic computational resources. It is critical for the ability to dynamically add resources. Virtualization software makes it possible to run multiple OS's and applications on a single server [37]. This software is known as a hypervisor or Virtual Machine Monitor (VMM). The aim of the VMM is to give access to each of the VM's to the hardware resources of the server such that the VMs can operate in isolation.  

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth, height=.3\textheight]{Figures/Virtual_Machine_Architecture.PNG}
\caption{Virtualization architecture [38]}
\end{figure}

\section{Machine Learning}
Machine Learning is a very promising area that has generate a lot of hype in the last decade. The concept has however been around for a long time with Arthur Samuel giving it the following definition in 1959: "Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed" [39]. This essentially means that the computer can learn without instruction by looking at data. There are a huge number of applications in modern society where Machine Learning is being utilized with excellent results. Some applications include email spam filtering where the algorithm will learn from a series of emails that have been labelled as containing spam or not. The algorithm would then be able to look at a new unlabelled email and utilize the information it has learned from the previous examples to classify this new email as spam or not. This task is known as classification, where there are only two outcomes which are essentially equal to true or false. Some other applications include detecting tumours in brain scans, detecting product damage on the assembly line, flagging offensive comments on social media and creating products that interpret human speech and respond e.g. Alexa. 

This project can be seen as an optimization problem. The goal is to essentially come up with a VM Selection model that will maximise server utilization while also making sure that the SLA's are adhered to. Therefore, we can view this is an optimization problem as we are trying to come up with the optimal placement of the VMs such that the requirements are met. Machine Learning approaches, therefore could utilize the data available from the data centre in order to optimize VM placement. 

\subsection{Supervised Learning}
In this approach to Machine Learning, the training set that is provided for the algorithm to learn from, has labels associated with each sample. For example if the objective was to differentiate if an email was spam or not, the training set would consist of the email and a label to say whether that email was in fact spam or not. The training data would typically be in vector format as Machine Learning algorithms work with numbers. The label is also known as the target output as in some instances we can remove this label, feed it to the algorithm after training and then test to see whether the algorithm correctly classifies the sample or not. The algorithm can output a number such as a house price and this is called regression. A label can also be outputted such as spam and this is known as classification. 

\subsection{Unsupervised Learning}
Unsupervised Learning means that the training data that is used to train the algorithm does not have any labels attached to it. The training set is simply made up of a series of input vectors with no outputs attached to each sample. The aim here typically is to put the input data into subsets. In doing so the algorithm will learn a set of rules for putting each sample into a subset and therefore it can apply these rules to new samples. One example of such an approach would be for a company to use clustering to put their customers into different categories. This may enable the company to learn more about the different groups that make up their customer base. One example of a piece of information that could be used is the gender of their customers. 

\subsection{Reinforcement Learning}
This is very much a different machine learning approach which involves a learning system that is called an agent [39]. This agent can observe its environment, take actions depending on the state of the environment and can accumulate rewards for those actions. These rewards could be positive for good actions or negative for poor actions. By observing positive and negative rewards the agent will learn a strategy, also called the a policy, which will achieve the highest reward over time. This policy will be what selects the agents next action when it is in a given state in the environment. 

\subsection{Agents}
In the area of agents in Reinforcement Learning there are many different definitions that depend on the domain that agent is being applied in. Russell $&$ Norvig states that "an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators" [40]. This definition highlights the fact that the agent has the ability to sense what is happening in its environment. The agent is then able to make a decision about what action to take based on a goal embedded in the agent. The aim is to build up an understanding of its environment such that in any given state the agent can make a decision that it knows will lead to the agents goal. One simple example of an agent is a thermostat. The thermostat is constantly checking the state of the environment through its temperature sensors. The goal of the thermostat is to keep the temperature in a given range. Therefore, based on the current environment state the agent will take action. If the temperature falls outside the range the agent will turn the heating off and if is above the range it will turn on the air-conditioning. The agent will take actions to ensure that its goal is reached. 

An agent must have the following three characteristics. Agents must be reactive, proactive and social. Reactive in the sense that they should be constantly monitoring their environment such that they are ready to act in a timely manner in the event of an environmental change. They must be proactive such that when an agent reacts to a change in their environment, it must be with the best interests of reaching their goal. This means an agent will not knowingly choose an action with poor reward when an action of high reward is available. An agent must finally be social, meaning that agents must be able to co-exist with other agents who may be in their environment with common or conflicting goals. Agents will therefore need to be able to cooperate and communicate with one another. 
  
\subsection{Reward Based Learning}
The process of an agent learning through rewards, or reward based learning, is illustrated in figure 2.8. The idea of a reward is an extension of the agents goal. 
\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.3\textheight]{Figures/Agent_Environment.png}
\caption{Agent Environment Interaction [41]}
\end{figure}
With a goal the agent will simply only know if it has reached its goal or not and therefore can only class states as being bad or good based off this. The reward however, introduces a performance measure that the agent can use to say how good a new state is. For example, a state may give high reward because it may be close to the agents goal. The agent can utilize this reward to determine what a good action it is to take in a certain state. 


\subsection{Multi-Agent Systems}
In many applications where agents are deployed there is oftentimes numerous other agents in the environment. This could be see as a society of agents with each agent having its own goals that can be in agreement or conflict with those of other agents [42]. An Agent in such a system, typically only has partial information available of its environment as it only typically will communicate to the agents that are close by [43]. The action of each agent has consequences on the environment, especially for its neighbouring agents [44]. This means that agents can be in conflict as the action of one agent can negatively affects the goal of another agent. Agents can also be dependent on one another as dependency relationships can form. These can take many different forms. There can be a one way dependency where one agent depends on the other while the other agent does not depend on that agent to achieve its goal. There can also be a mutual dependency where both agents depend on one another to achieve their goals [42]. This is in fact an ideal scenario as it encourages both agents to cooperate. Cooperation is a key concept in multi-agent systems and involves the agents sharing common goals and working together to achieve those goals. To do so agents must not only take action to achieve their goals but to also recognize the goals of other agents and help achieve them [45]. Cooperation enables a large task to be broken down into several tasks, each of which can be done by an agent. The agents can then cooperate together in parallel to carry out the larger tasks. Completely cooperative agents can change their goals to help other agents while Antagonistic agents will not cooperate preventing both agents from achieving their goals [46]. 

 
\subsection{Markov Decision Process}
A control problem is one where the policy is not fixed and thus the goal is to find the optimal policy. The optimal policy is the one that will lead to the highest reward in all states. There may be many optimal policies but each policy leads to the same optimal value function. The Markov Decision Process or MDP provides a structure by which decision making can be understood. It provides a framework for controlling systems that develop in a stochastic manner [48]. The Markov states that the next state is only dependent on the current state and therefore can be seen as memoryless, as the past states do not affect the next state. The MDP is represented as a 4 tuple, consisting of states, actions, transition probabilities and rewards [49]. 
  
\begin{itemize}

    \item \mathcal{S}, represents the set of all possible states;
    \item \mathcal{A}, symbolizes the set of all actions;
    \item $\mathscr{p(s_{t+1}|s_t,a_t)}$, represents the probabilities of the state transitions i.e. the probability of transitioning from one state to the next;
    \item $\mathscr{q(s_{t+1}|s_t,a_t)}$, represents the reward received from taking a specific action in a state;
\end{itemize}

\mathcal{S}, is the set of all of the possible states the agents could be in. After a time period $\mathscr{t}$, the agent will be in one of those states. The agent must then choose one of the available actions from the set of all possible actions \mathcal{A} [50]. The execution of this action will then lead the the agent transitioning to the next state and they will get a reward for that action. There is also a transition probability and this probability will determine if the agent moves on to the next state or not. In the case where the environment is fully observable, then we have all information available to the agent and no approximation is required. This means that a process known as value iteration can be utilized to find the optimal policy to the MDP.  

\subsection{Curse of Dimensionality}
Reinforcement Learning algorithms suffer from a concept known as the Curse of Dimensionality. This is caused due to the presence of Q-Values. These values store act as the memory the agent has of the environment. Each state and action pair is represented by a Q-Value. These Q-Values must be stored and iterated through in search of particular values to return or update them. For each state there are n actions and each action value must be stored for each state. This means that if we have 100 states and we add 1 new action value, we increase the size of the Q-Value table by 100 values. Therefore, it is critical to keep the number of states and actions to a minimum. Large state-action spaces can cause the algorithm to become very slow in decision making and thus, must be avoided. 

\section{Reinforcement Learning Algorithms}
\subsection{Temporal Difference Learning}
One of the main properties of Temporal Difference Learning (TDL) is that it is a model-free based form of learning. This means that it does not required the full set of information outlined in the MDP process. TDL algorithms work without the transient probabilities. TDL enables an agent to learn directly from its environment, without any prior knowledge. The aim of RL is to converge on an optimal solution. TDL aims to arrive at this optimum by incrementally modifying the utility of a state-action pair by utilizing the feedback given from the environment. There are two main TDL algorithms, Q-Learning and SARSA.

\subsection{Q-Learning}
Q-Learning is a TDL algorithm which stores its knowledge of the environment in a state-action table also known as a Q-Value table. This table is essentially a matrix of values which is updated after each time step. In each time step the agent will be in a state, take an action and observe some feedback for that action from the environment, through a reward. This enables the algorithm to update the matrix table to include this environmental feedback, which then increases the knowledge of the agent. The agent then has a better understanding of what action to take the next time it enters that state. The Q-Learning algorithm will take an action based on the selection policy being utilized, such as $\epsilon$-greedy or Softmax. Once the agent takes this action it will observe the new state and the feedback from the environment, also known as a reward. The agent will then update its understanding of the environment by utilizing equation 2.1 below. The old Q-Value for the state is updated to include the new environmental information that has been learned. One of the key aspects of Q-Learning is that it looks at the next state and loops through all the action values in the next state and will take the maximum Q-Value from this list and include this in the update for the old Q-Value. It is important to note there are two hyper-parameters Alpha and Gamma. Alpha is used to tune how much emphasis is put on the new information learned, A low Alpha means we prioritise this new information less. Alpha is also referred to as the learning rate. Gamma on the other hand influences how much emphasis is placed on the maximum Q-Value available in the next state. Thus, Gamma it defines the level of importance placed on the future available rewards. A low Gamma means more focus is placed on the current reward. The best way to find the optimal hyper-parameters in Q-Learning is through testing as they are scenario dependent. 

\begin{equation} \label{eqn}
Q(s{_t}, a) \gets Q(s, a) + \alpha [r + \gamma \max_{a} Q(s{_{t+1}}, a) - Q(s, a)]
\end{equation}

\begin{algorithm}
\caption{Q-Learning Algorithm}
\begin{algorithmic}
\STATE \textbf{Initialize} Q-Values, for all state-action pairs
\FOR{episode in Episodes}
    \STATE \textbf{Initialize} state
    \FOR{each step in episode}
    \STATE \textbf{Choose} $A$ from $S$ using chosen policy (e.g. $\epsilon$-greedy)
    \STATE \textbf{Take} action $A$
    \STATE \textbf{Observe} Reward $R$ and new State $S'$
    \STATE \textbf{Update} $Q(S, A) \gets Q(S, A) + \alpha [R + \gamma \max_{A'} Q(S', A') - Q(S, A)]$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{SARSA}
SARSA is a modified version of the Q-Learning algorithm created by Rummery & Niranjan in 1994 [XX]. SARSA can be called an on-policy algorithm as it will always follow its policy both when choosing an action and updating the Q-Value. This is not the case with Q-Learning. Q-Learning will update the Q-Value with the maximum Q-Value from the next state, therefore not following the policy. SARSA will carry out an identical process to Q-Learning until the step to update the Q-Value. When SARSA reaches the next state, it will use the policy to select a new action. It does not take this action however, it will utilize the Q-Value for the new state and new action pair to update the previous Q-Value. This is highlighted in equation 2.2 below. SARSA also has the hyper-parameters alpha and gamma which are utilized in the same fashion as in Q-Learning. 

\begin{equation} \label{eqn}
Q(s{_t}, a) \gets Q(s, a) + \alpha [r + \gamma Q(s{_{t+1}}, a{_{t+1}}) - Q(s, a)]
\end{equation}

\begin{algorithm}
\caption{SARSA Algorithm}
\begin{algorithmic}
\STATE \textbf{Initialize} Q-Values, for all state-action pairs
\FOR{episode in Episodes}
    \STATE \textbf{Initialize} state
    \FOR{each step in episode}
    \STATE \textbf{Choose} $A$ from $S$ using chosen policy (e.g. $\epsilon$-greedy)
    \STATE \textbf{Take} action $A$
    \STATE \textbf{Observe} Reward $R$ and new State $S'$
    \STATE \textbf{Choose} $A'$ from $S'$ using chosen policy (e.g. $\epsilon$-greedy)
    \STATE \textbf{Update} $Q(S, A) \gets Q(S, A) + \alpha [R + \gamma Q(S', A') - Q(S, A)]$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{RL Action Selection Policies}
\subsection{Exploration Vs Exploitation}
Exploitation means taking actions that have been previously taken, which are known to likely give good reward. Such a policy that does no exploration is called a greedy selection policy. However, if we are to stick to taking such exploitative actions, there may be actions that will not be taken which could lead to even better rewards. This introduces the requirement to take exploitative actions to fully understand the environment. Finding the right balance in this trade-off can be difficult as the longer the algorithm runs, the more it will have explored the environment meaning less explorative actions are required. Typically, algorithms explore the environment by taking a random action with a probability n for each action. An example of this would be a probability of 0.1, meaning on average one in ten actions are random. This probability may then be marginally reduced with each run such that as the algorithm runs longer, less random actions are taken until eventually the agent has its optimal policy and will take the action in line with the policy each time. 

\subsection{$\epsilon$-greedy}
$\epsilon$-greedy is a policy which utilizes exploration and exploitation. The algorithm starts out with a value $\epsilon$ which represents the probability that a random action will be taken. For example an epsilon value of 0.1, means a random action will be taken on average one in ten times. This value can be reduced with each iteration to reduce exploration. With $\epsilon$-greedy all actions available have the same probability of being chosen, this can be seen as a disadvantage as one could argue an action that has a higher Q-Value should be taken more often when compared to a lower value action.

\subsection{Softmax}
Softmax like $\epsilon$-greedy has a probability value for a random action. However, this time each action does not have the same probability of being chosen. Actions are given a probability based on their Q-Value. This essentially means that if an action is known to give a good reward, it is more likely to be picked. Therefore, it is more likely to pick a random action that will lead to a good reward while also randomly selecting actions that may lead to poor rewards. Softmax and $\epsilon$-greedy each have scenarios where one improves on the other, therefore both should be tried in the specific application. 


\chapter{Literature Review}
Due to the inefficiencies present in cloud computing, a large amount of research has been carried out to enable data centres to become more efficient, while also adhering to the SLA's present. Approaches aim to maximize the utilization of resources such that the providers maximise their revenue. Such an approach will lower energy costs and therefore, provide numerous advantages to both the provider and society. However, the dynamic nature of cloud computing and its demand means that the problems of VM Selection and Placement are highly difficult. Much research has been carried out in the areas of energy efficiency and dynamic resource allocation. The research carried out can be placed under one of the following three headings:

\begin{itemize}
    \item Threshold Based Approaches;
    \item Artificial Intelligence Based Approaches;
    \item Reinforcement Learning Based Approaches;
\end{itemize}

\section{Threshold Based Approaches}
Lee at al. recognized that a significant portion of the energy consumed by servers in cloud computing environments, was spent on under-utilized resources [51]. They concluded that while it is positive to have free resources, they still utilize power while idle. With this in mind they concluded that a resource allocation policy that takes under-utilized resources into account would lead to reduced energy consumption. Thus they proposed two heuristics, ECTC and MaxUtil, that would reduce the energy consumption of underutilized resources. The goal of these heuristics is to maximise utilization thus reducing the energy consumed. Both heuristics take into account the active and idle energy consumption. They assign each new task to the resources that will require the least energy utilization. This is where the energy savings are made. The two heuristics differ on how they make their decision with MaxUtil taking the average and ECTC actually calculates the energy consumed. The results are impressive especially with the ECTC algorithm which achieves a saving of 18\%.

The idea of costs of under-utilized resources is further studied by Srikantaiah et al. as they firstly studied the relationship between energy consumption and under-utilized resources [52]. The study conducted also took into account the performance levels, and involved looking at how these three factors impacted one another. The research looked at this relationship when workloads were consolidated on one host. They then proposed an algorithm that aimed to find the allocation of workloads to servers that would result in the minimum energy utilization. They found that this minimum usage of energy was found at a specific level of utilization and performance. This performance would likely not be high enough to work with the SLAs present on servers and hence the authors modified the algorithm such that the it now minimized the energy consumed with a performance constraint. Ts modification meant the SLAs were obeyed. If a server no longer can take any new requests, as it has reached the SLA, a new server is started up. One flaw with this approach is that it assumes that each application can be hosted by all servers, which is not the case. 

Beloglazov et al. then while carrying out similar work breaks the problem into two sub problems. These problems are VM Placement and VM Selection. VM Placement is the process of identifying the best possible server placement for the VMs. To do this they propose an algorithm called Modified Best Fit Decreasing (MBFD), based on the Best Fit Decreasing (BFD) algorithm. This involves a two step process where firstly, all of the VM's are sorted in decreasing order of their current CPU utilization. Next each VM is allocated to the host that will result in the least increase in power consumption. This would then go on to be named the Power Aware Best-Fit Decreasing algorithm (PABFD). The next step is VM Selection, the process of determining which VMs to offload in the case where a server is about to be or is overloaded. Three VM Selection policies were proposed in this research. The first, the Minimization of Migration (MM) policy, involves choosing the minimum number of VMs that need to be migrated for the server utilization to go below the threshold. The next policy is called the highest potential growth (HPG) policy. This policy will migrate the VMs that are utilizing the lowest amount of CPU usage relative to the CPU capacity, as this will reduce the risk of a large increase in CPU utilization for the host the VM is migrating to. The final policy, the Random policy, randomly selects a number of VMs to migrate. It is important to note that VM Selection is only carried out when the server's CPU usage is above a threshold which if left would lead to violation of the SLAs in place. Each of these proposed policies was evaluated using a cloud computing simulation framework, CloudSim. Service Level Violations (SLAVSs), number of migrations and power consumption were the criteria for evaluation. It was found the the MM policy was the best performer across each of the three criteria. 

Further research again carried out by Beloglazov et al. again provided advancements in the literature. In this research they carry out a study which determine that VM placement should be carried out continuously in an online manner [53]. The goal of this experiment was to provide more dynamic thresholds that could deal with the unpredictable nature of the workloads. This research provides new heuristics that enable better detection of host over-utilization. Local Regression (LR), a technique that had been present in the research community for some time, was utilized. This method involves using a subset of data such that a curve can be drawn that approximates the original data. Then using this curve the trend line can be drawn and a prediction can be made as to whether the host is likely to soon become over-utilized. A novel VM selection policy called Minimum Migration Time (MMT) is also introduced. This policy selects the VM that will lead to the least amount of migration time and migrates that VM. This research combined the LR algorithm (detect host over-utilization),the MMT (VM selection policy) and the PABFD algorithm (VM allocation policy). Their research concluded that these three algorithms together outperformed all other VM consolidation techniques and achieved large savings in energy consumption. 

Work carried out by Cardosa et al. took inspiration from virtualization technologies such as VMware and Xen. These technologies provide a maximum and minimum number of resources that can be allocated the the VMs [55]. Then the hypervisor will distribute the left over resources amongst all of the VMs. This enables an adaptable number of resources to be given to a VM based on the resources available, the cost of power and application utilities. This idea is created on the premise that not all applications are equal, with some being more important than other. During times of high CPU load, the CPU resources are best to be given to important applications. A CPU share ratio is then defined. If this ratio was to be 4:1, the scheduler will give high priority VMs 4 CPU cycles and 1 CPU cycle to low priority VMs. The authors then created a PowerExpandMinMax (PEMM) algorithm which is an improved extension on the ExpandMinMax (EMM) algorithm. Both of these algorithms aims to solve both the min-max and shares aware placement problems. Their implementation was then tested on synthetic data center setups along with a real data test bed. One experiment on the real data test bed demonstrated an impressive 47\% increase in data centre utility.

Kusic et al. aimed to provide a better solution to the difficulty of this sequential optimization problem under uncertainty, by utilizing a lookahead control scheme [55]. The algorithm, Limited Lookahead Control (LLC), aims to make cost savings by limiting CPU power usage, minimize SLAVs and maximize company profits. The approach utilizes a Kalman filter to estimate the incoming workload and, thus can distribute the resources accordingly. This prediction of workload greatly increases the computational complexity, which is flagged as a big concern by the researchers. An experiment was conducted to test the approach on a small test bed that consisted of 6 servers. These experiments concluded that their approach resulted in a 26\% reduction in energy consumption. 

\section{Artificial Intelligence Based Approaches}
The previous section looked at defined thresholds that once surpassed, would lead to some form of action to optimize the allocation of data centre resources. This section will evaluate research on approaches that utilize AI to optimize the problem of resource allocation in the Cloud Computing environment. 

Portaluri et al. proposed the use of a Genetic Algorithm (GA) to optimize the allocation of data centre resources and thus reduce the overall data centre power usage [56]. GAs are iterative optimization methods which are founded on the concepts of selection and evolution. This means that the potential solutions evolve towards better solutions by implementing these concepts. The paper takes a multi-objective approach to optimization as it aims to improve task completion time and reduce energy consumption. To do so an algorithm called the Non-dominated Sorting Genetic Algorithm II (NSGA-II) is utilized. This algorithm is widely applied to multi-objective optimization problems. This algorithm will discard any solution that is dominated by another solution. There will then be a number of equally valid solutions with a trade-off between lower energy cost and higher execution time. A solution can then be chosen based on this trade-off.

In 2015 Dashti et al. proposed the use of a modified version of the widely utilized optimization algorithm, Particle Swarm Optimization (PSO) [57]. The aim of the paper is to use this modified algorithm to dynamically optimize VM Placement such that the energy efficiency of the data centre is increased. VM Placement can also be viewed as a bin packing problem. The bins are the available servers, items are the VMs that have to be moved, bin sizes is the CPU utilization of the servers and the prices are the power consumption's of the nodes. The approach used Minimum Migration Time (MMT) as a metric to decide which VM is best to move from the over-utilized host. The host that this VM is to be placed is then chosen via the PSO algorithm. The algorithm calculates a value for each potential placement based on the increase in power consumption from the new placement. The placement with the least increase in power consumption is chosen. 

Wei et al. utilizes game theory as an approach to solving the problem of resource allocation [58]. Their approach consists of two independent steps. The first being that the participant solves its own problem independently. It does so by utilizing Integer Programming which will calculate a local solution which is independent of all other participants. The second phase utilizes an evolutionary optimization algorithm, which takes the results from stage one, and estimates an approximate optimal solution and divides the resources available. 

Berral et al. aimed to provide energy-aware scheduling in data centres by utilizing machine learning [59]. The research provides an intelligent consolidation methodology with techniques such as turning on and off servers, machine learning and power-aware consolidation algorithms. The aim of such techniques is to enable the algorithm to deal with information that has a degree of uncertainty while also optimizing performance. Their approach includes the use of a Supervised Machine Learning model which will predict the impact that workloads will have on the current available resources. This impact is examined via the energy consumed and the resulting performance level. This enables workloads to be re-allocated and servers that are under-utilized can be powered down. In the experiments carried out in this paper, their approach offered large improvements in both energy and performance efficiency. 

\section{Reinforcement Learning Based Approaches}
There are three methods by which Reinforcement Learning approaches can be utilized to optimize data centre operation. These include detection of under/over utilized hosts, VM Selection and VM Placement. These methods have been previously outlined in greater detail in previous sections. The following papers will utilize Reinforcement Learning to address one/some of these areas. 

Das et al. decided to take a multi-agent approach aiming to try and optimize the trade-off that exists between performance and power consumption [60]. Their approach aimed to turn off servers that were underutilized. This approach does not simply turn servers off when they are idle, like in many previous approaches, but it instead will turn servers off when they are have a low workload. This enables greater energy savings. Load balancing is also deployed such that tasks are rerouted away from underutilized servers to enable them to be turned off. 

Duggan et al. decided to take a Reinforcement Learning to VM Selection from hosts that became over-utilized [61]. The aim here was to have a RL agent choose the optimal VM to migrate in each scenario where the host was over-utilized. The Local Regression (LR) algorithm was utilized to predict whether a host was about to become over-utilized, signalling if a VM needed to be migrated. The researchers conducted numerous experiments in the CloudSim environment and the results were compared to the LR-MMT algorithm. The algorithms were compared using total energy consumption, number of SLAVs and the number of migrations across a number of different workloads. The experiments resulted in significantly improved performance in each of these three evaluation categories. 

Shaw et al. conducted similar research to that of Duggan et al. except this time focusing on VM Allocation/Placement [62]. The algorithm, referred to as the ARLCA algorithm, utilizes the Minimum Migration Time (MMT) for VM Selection. As done in Duggan et al., the experiments were set up in a very similar fashion with comparisons being made to the LR-MMT algorithm. Large reductions in the energy consumption, number of migrations and SLAVs were observed. 

Finally, research carried out by Barret et al. [50], introduced a Reinforcement Learning methodology for optimizing the required resources in a scalable manner. The aim of this research is to scale the resources available as they are required. The RL framework aims to determine an optimal scaling policy. The approach consists of local agents who estimate optimal policies and then share these observations with a global agent. 

\chapter{Simulation Environment - CloudSim}

\section{CloudSim Overview}
One of the main challenges of developing solutions that will improve data centre performance is the ability to test these models under varying conditions, in a repeatable manner. It must be possible to directly compare the performance of different algorithms under identical scenarios to accurately evaluate performance. CloudSim is a simulator developed in Java that has been created by the CLOUDS laboratory in the University of Melbourne [63]. It is an extendable simulation toolkit that enables modelling and simulation of the Cloud Computing environment. For the purpose of this research, CloudSim will enable the simulation of a data centre which will facilitate testing of this research's proposed algorithm. The toolkit also has the LR-MMT algorithm built in, which has become the benchmark in this field. A vast number of research papers in this area utilize the CloudSim toolkit. One of the main advantages provided is the ease at which a solution can be integrated into the toolkit. There are a vast number of interfaces which provide the foundations required to enable development of solutions to issues such as VM Selection and VM Placement. Over-utilized and under-utilized hosts are checked for by default every 300 seconds, which can be adapted. The setup also has host over-utilization prediction algorithms, such as Local Regression, built in. 

\section{Key CloudSim Classes}
The CloudSim Java package contains over 200 classes and interfaces. Below explanations have been included for some of the key classes outlined in the CloudSim paper [63].

\textbf{\textit{BwProvisioner}} - The is an abstract class that implements the policy for giving the bandwidth to the VMs. This component will distribute the bandwidth available to all of the competing VMs that are in the data centre. 
\vspace{\baselineskip}

\textbf{\textit{CloudCoordinator} }- This is an abstract class that will regularly monitor the state of data centre resources and thus will take action to reduce the load if required. 
\vspace{\baselineskip}

\textbf{\textit{Cloudlet}} - In CloudSim a Cloutlet represents the workload that has been given to a VM. This class facilitates the creation of a a Cloudlet object, it will monitor the location of the Cloudlet and it enables the stopping or cancellation of a Cloutlet from the the list of Cloudlet's. 
\vspace{\baselineskip}

\textbf{\textit{CloudletScheduler}} - This is responsible for the policy that will determine how processing power is shared amongst Cloutlet's in a VM. It can be space-shared or time-shared. 
\vspace{\baselineskip}

\textbf{\textit{Datacenter}} - Every Datacenter object creates a set of allocation policies for memory storage, bandwidth and storage devices to VMs and hosts. 
\vspace{\baselineskip}

\textbf{\textit{DatacenterBroker}} - The aim of this class is to act as a broker. It is essentially a middle man that handles communication between SaaS and Cloud suppliers, acting on the SaaS side. The broker is responsible for querying the CIS and will in turn allocate the resources required to meet the Quality of Service required for the application. The CIS will return a list of available VMs and their specification and such the broker can choose which VMs should be utilized. 
\vspace{\baselineskip}

\textbf{\textit{DatacenterCharacteristics}} - This defines all of the configurations/properties of the data center, e.g. the Operating System.
\vspace{\baselineskip}

\textbf{\textit{Host}} - This class represents a server which can host multiple VMs. It contains the total amount of memory and storage, a policy for how the processing power will be distributed amongst its VMs and policies for how the memory and bandwidth will be distributed to the VMs.  
\vspace{\baselineskip}

\textbf{\textit{VM}} - The will represent each of the VMs that are located on hosts. Each VM object has access to an object that stores the memory, storage, processor and the VMs provisioning policy. 

\vspace{\baselineskip}

\textbf{\textit{RamProvisioner}} - This is the class that depicts the policy that will distribute the available RAM between the active VMs. This will also ensure that a VM will only be placed on a host if it has the required memory available.  
\vspace{\baselineskip}

\textbf{\textit{SimEntity}} - This object represents the simulation that is being run. All simulations must therefore create an object of this class. There are three available actions in this class. The startEntity() method will start the simulation, the processEvent() method is repeatedly called once started in order to empty all the events in the queue called deferredQueue(). There is also a shutdown() method that will end the simulation, before it ends it will allow for some termination events such as printing the Log information to describe the simulation.  
\vspace{\baselineskip}

\textbf{\textit{VmSelectionPolicy}} - This abstract class provides the functionality required to decide what VM should be taken from the over-utilized host and relocated to a new host. This VMs new home will be chosen by the VmAllocationPolicy. 
\vspace{\baselineskip}

\textbf{\textit{VmAllocationPolicy}} - This abstract class provides the methods required for deciding what host a VM is placed on. The VMs that this policy will be moving host will come from hosts which are over-utilized. The VM that will be moved will be selected by the chosen VmSelectionPolicy. 

\section{Key CloudSim Information}
\subsection{Creating a New Energy Aware Policy}
The Energy Aware Policies are located in the \newline
org.cloudbus.cloudsim.examples.power.planetlab folder. These policies consist of a main method that when run will carry out the selected implementation. In this class the VM Allocation policy, VM Selection policy and the selected workload are defined. As outlined previously, the CloudSim package has a number of Energy Aware Policies available such as Lr-Mmt, Lr-Mc and Lr-Rs. To create a new policy, a new class must be created in this folder and the desired VM Selection and Allocation policies placed in the main method. All the relevant information is placed in a PlanetLabRunner object which is responsible for beginning the simulation. 

\subsection{Creating a New VM Selection/Allocation Policy}
Each of the VM Selection and Allocation policies are located in the \newline
/sources/org.cloudbus.cloudsim.power folder. It is important to note that when creating either of these types of policies, there are abstract classes for each. These classes detail the methods that are required for the new policies to work. The new class must simply extend the abstract class and then override the functionality of the abstract class. For example, the abstract method PowerVmSelectionPolicy contains the getVmToMigrate() method. This method has no functionality in the abstract class and an implementation must be created that will choose a VM to migrate from the host supplied. If this implementation is not supplied, no VM will be selected to be moved. 

\subsection{Hardware Setup and Workloads}
The CloudSim data center consists of 800 servers with two cores each, with each server having 4GB of RAM and 1GB of storage and bandwidth. CloudSim also has a number of different workloads that can be selected.  

\chapter{Reinforcement Learning Algorithm}

\section{Overview}
This chapter will detail each of the required steps to get the Reinforcement Learning Algorithm to operate within the CloudSim environment. These steps involved registering a new RL VM Selection policy and creating a number of new classes that enabled the RL agent to carry out VM Selection. 

This chapter will then detail the flow of the program i.e. how the RL agent is prompted to select a VM to move. Finally, this chapter will outline the state-action space, the reward process and the structure of the chosen algorithms (SARSA and Q-Learning). 

\section{Register RL VM Selection Policy}
When an Energy Aware Policy object, such as Lr-Mmt is run, the main method creates a PlanetLabRunner object, This object will take in the workload, VM Allocation and VM Selection policies chosen. This object extends an abstract class called RunnerAbstract. Each time this class is object is created it will call methods to get the VM Allocation and Selection policies, based on the information passed via the PlanetLabRunner. The policies passed are string names rather than objects and thus the new RL VM Selection algorithm must be added to this getVmSelectionPolicy() method. This method checks the string contents and then creates and returns a VM Selection object based on the input. Therefore, when the string "RL" is passed in the PlanetLabRunner as the VM Selection Policy, the method will create a PowerVmSelectionPolicyRL object. This process enables the CloudSim environment to recognize the newly created RL VM Selection policy. 


\section{Reinforcement Learning Additional Classes}
The following classes were added to the CloudSim toolkit to enable the RL VM Selection policy to work. There were also some modifications made to some existing classes, these will be detailed in subsequent sections. 

\subsection{PowerVmSelectionPolicyRL}
This class extends the abstract class PowerVmSelectionPolicy and therefore it must override the getVmToMigrate() method which will be called each time there is a host that is over-utilized. This means that this methods functionality will be called from the PowerVmSelectionPolicyRL class and therefore, the functionality in this class is critical for VM Selection. In this case, when the class is first created it creates objects of type Environment, Agent and Algorithm. When a host is over-utilized the getVmToMigrate() method is called to choose a VM to move to a new host. This method then calls the getAction() method in the Agent class. 

\subsection{Agent}
The Agent class has a couple of key methods, the main one being the getAction() method. This method will call the Migrate() method from the Algorithm class. It is important to note that the migration must happen first before the Q values are updated. This process is highlighted in the figure 5.1 below. Firstly a VM DataCenter Event is triggered every 300ms, then a check is done to see if there are any over-utilized hosts using the Local Regression algorithm. The algorithm will then loop over each of the over-utilized hosts. For each host the getVmToMigrate() method in the PowerVmSelectionPolicyRL class will call the agents class and this agent class will call the migrate method in the algorithm class. Once the migration has occured an event will be triggered. This event will call of the updateQValues() method in the PowerVmSelectionPolicyRL which will call the updateQValues() method in the Agent class. The algorithm will then update the Q-Value in a way that depends on the algorithm being utilized. A check will then be done to see if the host is still over-utilized and if it is the VM migration process will start again. This process is done for each of the over-utilized hosts. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth, height=.5\textheight]{Figures/Migration_Update.PNG}
\caption{Reinforcement Learning Algorithms Two Step Process - High Level}
\end{figure}

\subsection{Algorithm}
The Algorithm class contains all of the functionality for choosing which VM to migrate. The process of choosing a VM to migrate is identical for both the Q-Learning and SARSA algorithms. The process for updating the Q-values is different however, with the Algorithm class containing the functionality for both scenarios. The Algorithm class will obtain much of the required information from the Environment class such as the Q-values, reward, current state and next state.

\subsection{Environment}
The Environment class is responsible for storing all the values and functionality corresponding to the environment that the agent is in. The class will store the methods for calculating the reward, the state and it will also store the Q-values associated with each state-action pair. 

\subsection{LrRL}
This is the Energy Aware Policy for the RL agent. This class is placed in the same org.cloudbus.cloudsim.examples.power.planetlab folder along with all of the other Energy Aware Policies. The LrRl class consists of a main method that creates a PlanetLabRunner object. The main information given to this object is the chosen workload to be used and the VM Allocation and Selection Policies. For this policy we select the Selection Policy to be "RL" which signifies Reinforcement Learning and the Allocation policy to be "Lr", which represents Local Regression. Running this class will run the simulation with the selected VM Selection and VM Allocation policies on the chosen workload.

\section{VM Selection Process}
Figure 5.2 below details the process of how the algorithm selects a VM to migrate. This process is the identical regardless of whether the algorithm being used is Q-Learning or SARSA. The process is initiated with a "VM DataCenter" event which is triggered every 300 seconds. This triggers a series of methods which ultimately leads to looping over the list of over-utilized hosts. For each host over-utilized a VM is selected to be migrated by the RL algorithm. The algorithm chooses the VM through a process which will be detailed in a subsequent section. The algorithm then checks if the host is over-utilized post migration and if so it will again choose a VM to migrate. VMs are continuously migrated until the host is no longer over-run. This process is carried out until the list of over-utilized hosts is empty. Once this point is reached a "VM DataCenter" event is scheduled that will trigger the start of this process again in 300 seconds.

\begin{figure}[htpb!]
\centering
\includegraphics[width=.7\textwidth, height=.9\textheight]{Figures/Migration_Process.PNG}
\caption{VM Selection Process Flowchart.}
\end{figure}

\section{Q-Value Update Process}
This procedure is triggered by an event that is generated after each completion of a migration of a VM from an over-utilized host. The process is illustrated in figure 5.4. A series of methods are called after the event is generated and a method called updateQValues() is called from the Agent class. A check is then carried out in the Algorithm class to check if the algorithm being used is Q-Learning or SARSA. A different update method is then called in the environment class to update the Q-Value based on selected algorithm. Finally, once the Q-Value has been updated the process is complete and the algorithm returns. 

\begin{figure}[htpb!]
\centering
\includegraphics[width=.6\textwidth, height=.9\textheight]{Figures/Update_Process.PNG}
\caption{Q-Value Update Process.}
\end{figure}

\subsection{State-Action Space}
When defining the state-action space in RL algorithms it is important to recall from previous chapters, that this space suffers from the Curse of Dimensionality. Meaning that for each new state or action added the number of Q-Values added is exponential. When taking actions or updating values this space will have to be iterated and updated. Therefore, it is critical to keep the search space as small as possible to avoid the agent taking too long to make decisions. The selected state space used in this implementation is the current host utilization. This is equal to the sum of requested CPU usage in Mips of the VMs on the host, divided by the total host Mips. This value is then multiplied by 100 giving it a range of between 0-100. Sometimes the host can be highly over-run, giving a state value above 100, therefore 120 states were created, one for each percentage. Since the states utilized are whole numbers the returned state is rounded to the nearest whole number. In the state equation below, s represents the state, n represents the number of VMs on the host, Vmu represents the utilization on the VM and Hu represents the total host utilization. 


\begin{gather*}
s = \frac{{\sum_{n=1}^{n} Vmu(n)}}{Hu} \cdot{100}
\end{gather*}

An action is then defined as the as the VMs utilization divided by the sum of utilization of all of the VMs on the host. Therefore, it is the fraction of host utilization utilized by that VM. This value again multiplied by 100 and rounded to the nearest whole number to give a range of 0-100. In the equation below a represents the action, Vm represents the utilization of the VM we are getting the action value of, n represents all the VMs on the host and Vmu represents the utilization of that VM. 

\begin{gather*}
a = \frac{{Vm}}{\sum_{n=1}^{n} Vmu(n)} \cdot{100}
\end{gather*}

\subsection{Reward}
The reward encourages the agent to take the actions resulting in lower data center energy consumption while discourage actions that increase it. The strategy implemented hypothesized that fewer migrations would lead to less VMs being migrated and therefore, reduced power consumption. Thus, the reward aimed to reward actions that were high in value, meaning that large VMs were being moved. This meant that instead of 3 small VMs being moved, one large one would be moved. This resulted in less migrations and energy savings. As a result the host would spend less time over-utilized and therefore an improvement in SLAVs was anticipated. Clever VM Placement would also lead to less under-utilized hosts. Finally, the over-utilized host would be far less likely to become over-utilized again in the near future as a large CPU consumer has been moved. 

\section{Q-Learning Algorithm}
The Q-Learning algorithm starts by looping over each of the hosts present in this list of over-utilizd hosts. The next sequence of steps will be looped over until the host is deemed to be no longer over-utilized. The current state of the environment is observed. Next, all of the VMs on the host are iterated over and their action values are calculated and added to a list. This list is then iterated over and the Q-Value is looked up for each current state-action pair with each value being added to a list. The action value is then chosen with the highest Q-value. If there are numerous VMs with the same Q-Value, one is selected at random with equal probability. This VM is then chosen as the VM to migrate. Once the migration is complete, the next state and reward for the action are observed. The maximum Q-Value for the next state is then observed. The new Q-Value is then calculated and inputted into the Q-Value table in place of the old value.
\begin{algorithm}
\renewcommand{\thealgorithm}{}
\caption{Q-Learning Algorithm}
\begin{algorithmic}
\FORALL{Hosts in Over-Utilized Host List}
    \WHILE{Host is Over-Utilized}
        \STATE \textbf{Observe} Current State
        \FORALL{VMs on the Host}
            \STATE \textbf{Get} Action Value for VM
        \ENDFOR
        \FORALL{VM Action Values}
            \STATE \textbf{Lookup} Q-Value for (Current State, Action) Pair
        \ENDFOR
        \STATE \textbf{Choose} VM with highest Q-Value
        \STATE \textbf{Migrate} VM
        \STATE \textbf{Observe} Next State, Reward
        \STATE \textbf{Get} maxQ - The maximum Q-Value in the Next State
        \STATE \textbf{Calculate} newQ = oldQ + alpha * [reward + gamma * maxQ - oldQ]
        \STATE \textbf{Update} Old Q-Value with New Q-Value
    \ENDWHILE
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{SARSA Algorithm}

The SARSA algorithm behaves identically to the Q-Learning algorithm up until the step of calculating the new Q-Value. Instead of taking the maximum Q-Value from the next state, the SARSA algorithm again follows the policy and selects a new action from the new state. This process of selecting action/VM to migrate is identical to the previous selection however, this time the migration is not carried out. The agent this time observes the Q-Value for the new state and new action pair and inputs this value into the equation to calculate the new Q-Value. This difference is the key differentiator between Q-Learning, which takes the maximum Q-Value from the next state, and SARSA. 

\begin{algorithm}
\renewcommand{\thealgorithm}{}
\caption{SARSA Algorithm}
\begin{algorithmic}
\FORALL{Hosts in Over-Utilized Host List}
    \WHILE{Host is Over-Utilized}
        \STATE \textbf{Observe} Current State
        \FORALL{VMs on the Host}
            \STATE {Get} Action Value for VM
        \ENDFOR
        \FORALL{VM Action Values}
            \STATE \textbf{Lookup} Q-Value for (Current State, Action) Pair
        \ENDFOR
        \STATE \textbf{Choose} VM with highest Q-Value
        \STATE \textbf{Migrate} VM
        \STATE \textbf{Observe} Next State, Reward
        \FORALL{VMs on the Host}
            \STATE \textbf{Get} Action Value for VM   
        \ENDFOR
        \STATE \textbf{Choose} VM with highest Q-Value
        \STATE \textbf{Get} QVal - The Q-Value of the (Next State, Next Action) Pair
        \STATE \textbf{Calculate} newQ = oldQ + alpha * [reward + gamma * QVal - oldQ]
        \STATE \textbf{Update} Old Q-Value with New Q-Value
    \ENDWHILE
\ENDFOR
\end{algorithmic}
\end{algorithm}

\chapter{Experiment Evaluation Metrics}
\section{Overview}
This chapter details the metrics utilized to evaluate the performance of the Q-Learning and SARSA algorithms, along with the different hyper-parameter combinations within each algorithms. By selecting a group of evaluation parameters, the performance of each algorithm can be evaluated and compared in a fair manner. These metrics have been widely adopted as the gold standard method of analysis of data center performance [54]. 

\section{Data Center Energy Consumption}
This is the total amount of energy utilized by the data center. This is a key metric as the aim of this research it to reduce the energy consumption of data centers. This metric is measure in kilo-Watt hours (kWh). 

\section{VM Migrations}
The approach taken in this these aimed to minimize the number of VM migrations by utilizing intelligent VM Selection. This means that this the total number of migrations is an excellent sign that the RL agent is behaving as intended. More migrations also lead to increased SLAV's. The total VM migrations and over-utilized host migrations are monitored. As the agent learns the number of migrations should fall. 

\section{Service Level Agreement Violations (SLAVs)}
A key element to operating a service on the cloud is ensuring the Service Level Agreements that have been put in place are adhered to. A reduction in energy consumption may reduce the Quality of Service (QOS) being provided. Therefore, this is a parameter that needs to be observed to ensure a high quality service is provided. This metric is a  accumulation of two other metrics recorder in Cloudsim, Performance Degradation Due to Migrations (PDM) and Service Level Agreement Violation Time Per Active Host (SLATAH). As shown in equation 6.1, the SLAV is calculated by multiplying the SLATAH by the PDM.

\begin{equation} \label{eqn}
SLAV =  SLATAH \cdot {PDM}
\end{equation}

\subsection{Performance Degradation Due to Migrations (PDM)}
This metric gives the overall drop in performance caused by carrying out migrations. This is calculated utilizing equation 6.2. M represents the number of VMs, $C_d_i$ represents an estimate of how much the performance of VM $i$ is reduced due to migrations and $C_r_i$ represents the lifetime requested CPU capacity by VM $i$. 

\begin{equation} \label{eqn}
PDM = \frac{1}{N}{\sum_{i=1}^{N}} \frac{C_d_i}{C_r_i}
\end{equation}

\subsection{Service Level Agreement Violation Time Per Active Host (SLATAH)}
This metric is equal to the total time that an active host experiences 100\% of CPU utilization. The metric is measure because should a host be at full capacity, the VMs are unlikely to reach the level of expected performance. The equation is presented in equation 6.3 below. H represents the number of hosts, $T_s_j$ is equal to the total time host $j$ spends at maximum utilization and $T_a_j$ is the total time that host is active. 

\begin{equation} \label{eqn}
SLATAH = \frac{1}{H}{\sum_{j=1}^{H}} \frac{T_s_j}{T_a_j}
\end{equation}

\section{Energy and SLAV - Combined Metric}
The energy consumption and SLAV are two essential metrics for evaluating the effect of the proposed algorithm on data center performance. However, these metrics typically have an inverse relationship, with a decrease in one will typically cause an increase in the other. Therefore, a metric that combines these two metrics will facilitate greater analysis of the algorithms performance. This combined metric, known as the ESV, is calculated by multiplying the energy consumption by the SLAV, as shown in equation 6.4. 

\begin{equation} \label{eqn}
ESV = EC \cdot SLAV
\end{equation}

\chapter{Hyper-parameter Analysis}
\section{Overview}
This chapter aims to answer the first two research questions posed in the introduction to this thesis. These questions pertain to obtaining the best Alpha and Gamma values for the task of VM Selection for the Q-Learning and SARSA algorithms. The aim of this experiment is to analyse the effects that these hyper-parameters, Alpha and Gamma, have on the performance of the Q-Learning and SARSA algorithms. This analysis of performance will be done utilizing the metrics for analysing data center performance, outlined in the previous chapter. The best Q-Learning and SARSA hyper-parameters will be selected for both action selection policies, $\epsilon$-greedy and Softmax. Once the best parameters have been selected, a more detailed analysis of the results achieved with these parameters will be carried out. 

\section{Experiment Outline}
To carry out this experiment 5 values were chosen for both Alpha and Gamma. This meant there are 25 unique combinations of hyper-parameters for each algorithm, The values chosen were 0.2, 0.4, 0.6, 0.8 and 1. Each parameter combination was run on a days workload 30 times, equivalent to a 30 day workload. The Q-Values resulting from one day were passed onto the next day. This meant that the agent was able to learn from the previous runs of the workload. The algorithm combinations are evaluated utilizing the 4 parameters outlined in the previous section. The average of each of these parameters was taken over the 30 days. 

\section{Q-Learning $\epsilon$-greedy}
Table 7.1 contains the results for the 25 different hyper-parameter configurations of the Q-Learning $\epsilon$-greedy algorithm, that were run for this experiment. The goal of each performance metric is to obtain a value as low as possible. The most critical metric used for evaluation is the ESV as this metric combines the energy consumption and the SLAVs. The results illustrate a definitive trend. A lower number of migrations tends to lead to lower energy consumption, less SLAVs and therefore a lower ESV score. Table 7.1 illustrates the effects that the hyper-parameters have on the results. The best configurations results have been highlighted in red, with the results of the two next best performing configuration placed in bold. The best configuration, results in the lowest of each of the evaluation parameters, obtaining an ESV score of 0.00933. While the second and third best configurations have similar values across each of the metrics, achieving ESV values of 0.00947 and 0.00958 respectively. It can be concluded that the best configuration for Q-Learning with an $\epsilon$-greedy action selection policy is with Alpha and Gamma values of 0.2. These results coupled with the other results achieved suggest that, while a lower Alpha value does improve performance, the Gamma value appears to be much more significant. The three best performing configurations have a Gamma value of 0.2, meaning a low Gamma value is critical.  

\begin{center}
\scalebox{0.65}{
\begin{tabular}{||c c c c c c||} 
 \hline
 \textbf{Alpha} & \textbf{Gamma} & \textbf{Energy Consumption} & \textbf{Total Migrations} & \textbf{SLAV} & \textbf{ESV} \\ [0.1ex] 
 \hline\hline
 \hline
\textcolor{red}{0.2}&\textcolor{red}{0.2}&\textcolor{red}{140.024}&\textcolor{red}{15815}&\textcolor{red}{0.00200}&\textcolor{red}{0.00933} \\
 \hline 
0.2&0.4&141.163&17543&0.00213&0.01001 \\
 \hline
0.2&0.6&142.177&19277&0.00218&0.01034 \\
 \hline
0.2&0.8&144.101&21052&0.00219&0.01053 \\
 \hline
0.2&1.0&151.056&23544&0.00204&0.01025 \\
 \hline
\textbf{0.4}&\textbf{0.2}&\textbf{140.165}&\textbf{16060}&\textbf{0.00203}&\textbf{0.00947} \\
 \hline
0.4&0.4&141.025&17630&0.00211&0.00992 \\
 \hline
0.4&0.6&142.315&19283&0.00218&0.01035 \\
 \hline
0.4&0.8&143.737&20886&0.00216&0.01035 \\
 \hline
0.4&1.0&151.175&23604&0.00200&0.01006 \\
 \hline
0.6&0.2&140.396&16311&0.00205&0.00961 \\
 \hline
0.6&0.4&141.147&17466&0.00212&0.01000 \\
 \hline
0.6&0.6&141.900&18939&0.00214&0.01014 \\
 \hline
0.6&0.8&143.524&20634&0.00220&0.01052 \\
 \hline
0.6&1.0&153.573&24434&0.00199&0.01018 \\
 \hline
\textbf{0.8}&\textbf{0.2}&\textbf{140.356}&\textbf{16350}&\textbf{0.00205}&\textbf{0.00958} \\
 \hline
0.8&0.4&141.411&17782&0.00215&0.01012 \\
 \hline
0.8&0.6&141.698&18515&0.00216&0.01018 \\
 \hline
0.8&0.8&142.831&19627&0.00218&0.01038 \\
 \hline
0.8&1.0&151.135&23614&0.00203&0.01023 \\
 \hline
1.0&0.2&140.489&16475&0.00206&0.00966 \\
 \hline
1.0&0.4&141.325&17694&0.00213&0.01005 \\
 \hline
1.0&0.6&141.904&18485&0.00217&0.01024 \\
 \hline
1.0&0.8&142.226&19397&0.00214&0.01016 \\
 \hline
1.0&1.0&148.198&22501&0.00206&0.01019 \\
 \hline
\end{tabular}}
\captionof{table}{Q-Learning $\epsilon$-greedy Results - Averaged Over 30 Day Workload}\label{QLeg}
\end{center}


\section{Q-Learning Softmax}
Table 7.2 depicts the results of the Q-Learning Softmax experiment. A trend very similar to that seen in the Q-Learning $\epsilon$-greedy results is observed, as a lower number of migrations leads to lower values for each of the evaluation parameters. Alpha and Gamma values of 0.2 again achieves the best result, achieving an ESV value of 0.00946. The same configurations achieve the second and third best results as they did in the previous experiment, achieving achieving an ESV of 0.00956 and 0.00975 respectively. This experiment, as in the previous experiment, concludes that a low Gamma value is critical. While a low Alpha value optimizes performance slightly, it is much less influential than gamma. 

\begin{center}
\scalebox{0.65}{
\begin{tabular}{||c c c c c c||} 
 \hline
 \textbf{Alpha} & \textbf{Gamma} & \textbf{Energy Consumption} & \textbf{Total Migrations} & \textbf{SLAV} & \textbf{ESV} \\ [0.1ex] 
 \hline\hline
 \hline
\textcolor{red}{0.2}&\textcolor{red}{0.2}&\textcolor{red}{140.256}&\textcolor{red}{16164}&\textcolor{red}{0.00202}&\textcolor{red}{0.00946} \\
 \hline
0.2&0.4&141.016&17559&0.00211&0.00991 \\
 \hline
0.2&0.6&142.607&19711&0.00221&0.01053 \\
 \hline
0.2&0.8&143.425&20427&0.00210&0.01006 \\
 \hline
0.2&1.0&150.314&23314&0.00210&0.01051 \\
 \hline
\textbf{0.4}&\textbf{0.2}&\textbf{140.356}&\textbf{16462}&\textbf{0.00204}&\textbf{0.00956} \\
 \hline
0.4&0.4&141.838&18514&0.00219&0.01034 \\
 \hline
0.4&0.6&142.801&19882&0.00220&0.01049 \\
 \hline
0.4&0.8&144.740&21440&0.00215&0.01036 \\
 \hline
0.4&1.0&151.119&23620&0.00204&0.01028 \\
 \hline
0.6&0.2&140.586&16672&0.00207&0.00970 \\
 \hline
0.6&0.4&142.362&19040&0.00220&0.01045 \\
 \hline
0.6&0.6&143.240&20276&0.00222&0.01060 \\
 \hline
0.6&0.8&144.863&21246&0.00211&0.01020 \\
 \hline
0.6&1.0&152.379&24082&0.00204&0.01038 \\
 \hline
\textbf{0.8}&\textbf{0.2}&\textbf{140.930}&\textbf{16947}&\textbf{0.00207}&\textbf{0.00975} \\
 \hline
0.8&0.4&142.903&20029&0.00222&0.01058 \\
 \hline
0.8&0.6&143.269&20172&0.00219&0.01047 \\
 \hline
0.8&0.8&144.696&21163&0.00217&0.01046 \\
 \hline
0.8&1.0&148.271&22601&0.00205&0.01015 \\
 \hline
1.0&0.2&141.163&17360&0.00212&0.00997 \\
 \hline
1.0&0.4&143.028&20011&0.00222&0.01059 \\
 \hline
1.0&0.6&143.739&20517&0.00222&0.01063 \\
 \hline
1.0&0.8&144.726&21199&0.00212&0.01023 \\
 \hline
1.0&1.0&147.883&22427&0.00206&0.01014 \\
 \hline
\end{tabular}}
\captionof{table}{Q-Learning Softmax Results - Averaged Over 30 Day Workload}\label{QS}
\end{center}

\section{SARSA $\epsilon$-greedy}
This experiment focused on the SARSA $\epsilon$-greedy algorithm, with the results being displayed in Table 7.3. In this case the top three results have almost identical ESV values of 0.009155, 0.009156 and 0.009158. Here a trade-off between energy consumption and SLAVs begins to emerge. As more migrations occur, less energy is consumed. However, this leads to greater SLAVs which highlights the importance of the ESV metric, as it takes both these metrics into account. The ESV shows in these three cases that the trade-off balances out giving almost identical performance. Again, it can be seen that a lower Gamma value is of high importance while the Alpha value is less influential. For this configuration, the algorithm is much more consistent with both Alpha and Gamma having much less influence on the performance. Changes in these hyper-parameters lead to much less degradation of performance when compared to that of either of the Q-Learning algorithms. 

\begin{center}
\scalebox{0.65}{
\begin{tabular}{||c c c c c c||} 
 \hline
 \textbf{Alpha} & \textbf{Gamma} & \textbf{Energy Consumption} & \textbf{Total Migrations} & \textbf{SLAV} & \textbf{ESV} \\ [0.1ex] 
 \hline\hline
 \hline
0.2&0.2&139.596&15265&0.0019790&0.009210 \\
 \hline
0.2&0.4&139.635&15370&0.0019919&0.009272 \\
 \hline
0.2&0.6&139.778&15516&0.0020067&0.009350 \\
 \hline
0.2&0.8&139.823&15515&0.0020036&0.009340 \\
 \hline
0.2&1.0&140.039&15714&0.0020365&0.009507 \\
 \hline
\textbf{0.4}&\textbf{0.2}&\textbf{139.451}&\textbf{15205}&\textbf{0.0019695}&\textbf{0.009156} \\
 \hline
0.4&0.4&139.539&15217&0.0019845&0.009231 \\
 \hline
0.4&0.6&139.605&15302&0.0019821&0.009225 \\
 \hline
0.4&0.8&139.952&15594&0.0020189&0.009420 \\
 \hline
0.4&1.0&140.226&15870&0.0020481&0.009575 \\
 \hline
\textbf{0.6}&\textbf{0.2}&\textbf{139.406}&\textbf{15210}&\textbf{0.0019707}&\textbf{0.009158} \\
 \hline
0.6&0.4&139.535&15235&0.0019779&0.009201 \\
 \hline
0.6&0.6&139.612&15351&0.0019955&0.009288 \\
 \hline
0.6&0.8&139.703&15508&0.0020118&0.009370 \\
 \hline
0.6&1.0&140.145&15934&0.0020548&0.009600 \\
 \hline
0.8&0.2&139.393&15246&0.0019756&0.009180 \\
 \hline
0.8&0.4&139.476&15255&0.0019818&0.009215 \\
 \hline
0.8&0.6&139.581&15314&0.0019773&0.009201 \\
 \hline
0.8&0.8&139.715&15417&0.0019987&0.009310 \\
 \hline
0.8&1.0&140.109&15841&0.0020480&0.009566 \\
 \hline
1.0&0.2&139.471&15224&0.0019708&0.009163 \\
 \hline
\textcolor{red}{1.0}&\textcolor{red}{0.4}&\textcolor{red}{139.518}&\textcolor{red}{15217}&\textcolor{red}{0.0019685}&\textcolor{red}{0.009155} \\
 \hline
1.0&0.6&139.538&15362&0.0019819&0.009220 \\
 \hline
1.0&0.8&139.695&15506&0.0020011&0.009319 \\
 \hline
1.0&1.0&139.869&15720&0.0020252&0.009444 \\
 \hline
\end{tabular}}
\captionof{table}{SARSA $\epsilon$-greedy Results - Averaged Over 30 Day Workload}\label{SE}
\end{center}

\section{SARSA Softmax}
Finally, table 7.4 shows the results for the SARSA softmax algorithm. These results again confirm the conclusions obtained in the previous experiments. More migrations tend to lead to higher energy consumption values. There are again a number of configurations with very similar ESV values. The best configuration obtains an ESV value of 0.009155 while second and third best achieves 0.009156 and 0.009158. This marginal difference is also present in the energy consumption, migrations and the SLAVs. As seen with the SARSA $\epsilon$-greedy algorithm, the performance is much more consistent regardless of Alpha and Gamma. 

\begin{center}
\scalebox{0.65}{
\begin{tabular}{||c c c c c c||} 
 \hline
 \textbf{Alpha} & \textbf{Gamma} & \textbf{Energy Consumption} & \textbf{Total Migrations} & \textbf{SLAV} & \textbf{ESV} \\ [0.1ex] 
 \hline\hline
 \hline
0.2&0.2&139.549&15263&0.00198&0.009197 \\
 \hline
0.2&0.4&139.580&15362&0.00199&0.009237 \\
 \hline
0.2&0.6&139.607&15349&0.00197&0.009178 \\
 \hline
0.2&0.8&139.923&15559&0.00200&0.009352 \\
 \hline
0.2&1.0&140.109&15756&0.00203&0.009473 \\
 \hline
0.4&0.2&139.509&15254&0.00198&0.009200 \\
 \hline
\textbf{0.4}&\textbf{0.4}&\textbf{139.507}&\textbf{15291}&\textbf{0.00197}&\textbf{0.009143} \\
 \hline
0.4&0.6&139.543&15352&0.00199&0.009241 \\
 \hline
0.4&0.8&139.758&15484&0.00201&0.009366 \\
 \hline
0.4&1.0&140.038&15753&0.00204&0.009542 \\
 \hline
0.6&0.2&139.418&15256&0.00197&0.009151 \\
 \hline
0.6&0.4&139.550&15293&0.00197&0.009169 \\
 \hline
0.6&0.6&139.554&15300&0.00198&0.009209 \\
 \hline
0.6&0.8&139.759&15478&0.00201&0.009346 \\
 \hline
0.6&1.0&139.944&15636&0.00202&0.009414 \\
 \hline
0.8&0.2&139.460&15257&0.00197&0.009160 \\
 \hline
0.8&0.4&139.495&15219&0.00197&0.009161 \\
 \hline
0.8&0.6&139.598&15293&0.00198&0.009207 \\
 \hline
0.8&0.8&139.635&15460&0.00201&0.009350 \\
 \hline
0.8&1.0&139.906&15564&0.00200&0.009320 \\
 \hline
\textbf{1.0}&\textbf{0.2}&\textbf{139.465}&\textbf{15199}&\textbf{0.00197}&\textbf{0.009142} \\
 \hline
\textcolor{red}{1.0}&\textcolor{red}{0.4}&\textcolor{red}{139.475}&\textcolor{red}{15193}&\textcolor{red}{0.00196}&\textcolor{red}{0.009101} \\
 \hline
1.0&0.6&139.584&15374&0.00198&0.009204 \\
 \hline
1.0&0.8&139.673&15442&0.00198&0.009226 \\
 \hline
1.0&1.0&139.923&15703&0.00202&0.009406 \\
 \hline
\end{tabular}}
\captionof{table}{SARSA Softmax Results - Averaged Over 30 Day Workload}\label{SS}
\end{center}

\section{Conclusion}
This chapter aimed to identifying the optimal algorithmic hyper-parameters, Alpha and Gamma, for VM Selection. This was carrried out for both Q-Learning and SARSA in an attempt to answer the first two research questions posed in the Introduction to this thesis. This involved conducting experiments on 2 different configurations of each algorithm. These configurations included Q-Learning $\epsilon$-greedy, Q-Learning Softmax, SARSA $\epsilon$-greedy and SARSA Softmax. The experiment concluded that in both the Q-Learning and SARSA cases, the optimal parameters for both configurations were the same. Table 7.5 outlines the optimal hyper-parameters for each algorithm. As concluded in this chapter, Gamma is highly influential on the Q-Learning algorithm, with a low value being optimal. Alpha has much less influence on the performance however, a lower value is optimal. SARSA is much less influenced by either parameter but the best performance is achieved with an Alpha of 1 and a Gamma value of 0.4. It is noteworthy that the SARSA algorithm performs in a much more consistent manner, with performance less influenced by the values of Alpha and Gamma. 

\begin{center}
\begin{tabular}{||c c c||} 
 \hline
 \textbf{Algorithm} & \textbf{Alpha} & \textbf{Gamma}\\ [0.2ex] 
 \hline\hline
 \hline
Q-Learning&0.2&0.2 \\
 \hline
SARSA&1.0&0.4 \\
 \hline
\end{tabular}
\captionof{table}{Algorithm Combinations and the Optimal Hyper-parameters}\label{SE}
\end{center}

\chapter{Q-Learning Vs. SARSA}
The aim of this chapter is to investigate whether Q-Learning or SARSA is the better performing algorithm when utilizing a Reinforcement Learning to VM Selection. Table 8.1 contains each algorithmic combination along with the best performing hyper-parameters for that configuration. In the subsequent sections each of these configurations will be evaluated across a 30 day workload. Each configuration is compared utilizing the performance measures outlined in chapter 6. The best performing algorithm will then be selected for comparison against the state-of-the-art algorithm, Lr-Mmt in the next chapter.

\begin{center}
\begin{tabular}{||c c c c||} 
 \hline
 \textbf{Algorithm} & \textbf{Policy} & \textbf{Alpha} & \textbf{Gamma}\\ [0.2ex] 
 \hline\hline
 \hline
Q-Learning&$\epsilon$-greedy&0.2&0.2 \\
 \hline
Q-Learning&Softmax&0.2&0.2 \\
 \hline
SARSA&$\epsilon$-greedy&1.0&0.4 \\
 \hline
SARSA&Softmax&1.0&0.4 \\
 \hline
\end{tabular}
\captionof{table}{Algorithm Combinations and the Optimal Hyper-parameters}\label{SE}
\end{center}

\section{Energy Consumption}
Figure 8.1 illustrates the energy consumption over the 30 workload for each algorithm. Initially, there is a large drop-off in energy consumption for each algorithm as the agent quickly learns what actions not to take. After this initial decrease the value fluctuates for each algorithm but the energy consumption is still trending in a downward direction. The SARSA algorithms are the best performers with the Softmax configuration slightly edging the $\epsilon$-greedy configuration. However, with Q-Learning the $\epsilon$-greedy algorithm has slightly lower energy consumption results. The SARSA $\epsilon$-greedy algorithm is most consistent performer with a standard deviation of 0.5622. Q-Learning Softmax was the next best performer with regards to consistency followed by Q-Learning $\epsilon$-greedy and SARSA Softmax with values of 0.627, 0.668 and 0.672 respectively. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/Energy_Consumption_Graph_All.PNG}
\caption{Energy Consumption Results for 30 Day Workload.}
\end{figure}

\section{Number of Migrations}
Figure 8.2 illustrates the results for the total number of migrations carried out for each day of the 30 day workload. Again, it can be clearly be seen that the number of migrations reduces as the agent learns from the previous workloads. Both SARSA algorithms again perform best however, this time their performance is difficult to separate. The Q-Learning $\epsilon$-greedy configuration outperforms that of the Softmax configuration. The SARSA $\epsilon$-greedy algorithm is most consistent performer with a standard deviation of 621.3. Q-Learning Softmax was the next best performer with regards to consistency followed by SARSA Softmax and Q-Learning $\epsilon$-greedy with values of 658.8, 731.1 and 820.2 respectively. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/Total_Migrations_Graph_All.PNG}
\caption{Total Migration Results for 30 Day Workload.}
\end{figure}

The breakdown of the over-utilized and under-utilized VM migrations have been included in figures 8.3 and 8.4. These have been included to illustrate that, not only does clever VM Selection from the agent reduce the number of over-utilized VMs that are migrated, but it also reduces the number of under-utilized VMs that need to be migrated. When considering this it is important to note that the agent only selects a VM to migrate when a host is over-utilized. Thus, one may think it will have no effect on under-utilized migrations, which is not the case as they are significantly reduced. This is due to the agent choosing large VMs to migrate which are presumably then placed on hosts that are under threat of becoming under-utilized. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/Over-Utilized_Migrations_Graph_All.PNG}
\caption{Over-Utilized VM Migration Results for 30 Day Workload.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/Under-Utilized_Migrations_Graph_All.PNG}
\caption{Under-Utilized VM Migration Results for 30 Day Workload.}
\end{figure}

\section{SLAVs}
The SLAVs for each algorithm over the 30 day workload is displayed in figure 8.5. The common trend previously observed can again be seen as the agent learns from more and more workloads. The SARSA Softmax configuration achieved the best results, which is especially evident in the last number of runs. This time however, it is much closer for the next best algorithm as the SARSA $\epsilon$-greedy and the Q-Learning $\epsilon$-greedy produce quite similar results. The SARSA $\epsilon$-greedy algorithm is more consistent and contains less fluctuations in value. Finally, as previously seen the Q-Learning Softmax configuration consistently has the largest numbers of SLAVs. The SARSA $\epsilon$-greedy algorithm is most consistent performer with a standard deviation of 1.44E-06. Q-Learning Softmax was the next best performer with regards to consistency followed by SARSA Softmax and Q-Learning $\epsilon$-greedy with values of 1.96661E-06, 2.25581E-06 and 2.44812E-06 respectively. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/SLAV_Graph_All.PNG}
\caption{Service Level Agreement Violations for 30 Day Workload.}
\end{figure}

\section{ESV}
The results of the most important metric, which combines the energy consumption and the SLAVs, is displayed in figure 8.6. As expected the SARSA Softmax configuration has the lowest ESV value, as it ranks best in both the energy consumption and SLAVs. Again it is difficult to choose the second best algorithm as both SARSA $\epsilon$-greedy and Q-Learning $\epsilon$-greedy have similar ESV values however, the former is much more consistent. Finally, the Q-Learning Softmax is the worst performing as expected. The SARSA $\epsilon$-greedy algorithm is most consistent performer with a standard deviation of 0.0002313. Q-Learning Softmax was the next best performer with regards to consistency followed by SARSA Softmax and Q-Learning $\epsilon$-greedy with values of 0.0003102, 0.0003583 and 0.0003805 respectively.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.45\textheight]{Figures/ESV_Graph_All.PNG}
\caption{ESV Results for 30 Day Workload.}
\end{figure}

\newpage

\section{Conclusion}
This section aimed to answer the third research question posed in the Introduction, by comparing the results of the best performing Q-Learning and SARSA algorithms to identify the best performing algorithm. The experiment aimed to do a detailed analysis of each algorithmic configuration, with their optimal hyper-parameters, to see what configuration gave the best performance. The results of the experiment conclude that the SARSA Softmax achieved the best results with SARSA $\epsilon$-greedy, Q-Learning $\epsilon$-greedy and Q-Learning ranking second third and fourth best respectively. It is important to note that this ranking does not take standard deviation into account. The reason for this is that the agent is taking random actions for a large proportion of the runs and only ends doing so in the last number of runs. Therefore, these are the most critical runs for evaluating which algorithms performs best. These results conclude that the SARSA Softmax algorithm is the best performing algorithm for the task of VM Selection.In the next chapter this algorithm will be bench-marked against the state-of-the-art VM selection algorithm, Lr-Mmt. 

\begin{center}
\begin{tabular}{||c c c c c||} 
 \hline
 \textbf{Algorithm} & \textbf{Policy} & \textbf{Alpha} & \textbf{Gamma}& \textbf{Rank}\\ [0.2ex] 
 \hline\hline
 \hline
Q-Learning&$\epsilon$-greedy&0.2&0.2&3 \\
 \hline
Q-Learning&Softmax&0.2&0.2&4 \\
 \hline
SARSA&$\epsilon$-greedy&1.0&0.4&2 \\
 \hline
\textcolor{red}{SARSA}&\textcolor{red}{Softmax}&\textcolor{red}{1.0}&\textcolor{red}{0.4}&\textcolor{red}{1} \\
 \hline
\end{tabular}
\captionof{table}{Algorithm Combination Ranking Results}\label{SE}
\end{center}

\chapter{Lr-RL Vs. Lr-Mmt}
\section{Overview}
The aim of this chapter is to answer the final research question posed in the Introduction to this thesis, by comparing the performance of the RL algorithm to that of the state-of-the-art Lr-Mmt algorithm. To carry out this experiment the best configuration, established in the previous chapter, was taken and trained a number of times on the workloads it was about to encounter. Both the Lr-Mmt and Lr-RL algorithms were then run on a set of 30 different workloads. The subsequent sections carry out detailed analysis of both algorithms across the evaluation parameters utilized thus far in this thesis. 

\section{Energy Consumption}
Figure 9.1 illustrates the energy consumption values of both algorithms across the 30 day workload. This figure clearly shows that the Lr-RL algorithm is far superior in terms of the energy consumed across each of the workloads. Utilizing the Lr-RL algorithm results in a 399.12kWh energy saving over the 30 days, a reduction of 13.304kWh per day. This results in on average a 10.37\% daily saving in energy consumption. The paired t test concludes that there is a significant statistical difference in energy consumption, returning a two-tailed P value of less than 0.0001 with a 95\% confidence interval (-14.0790, -12.5290).

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.5\textheight]{Figures/Lr_RL-Vs_Lr_Mmt_Energy_Consumption.PNG}
\caption{Lr-RL Vs. Lr-Mmt - Energy Consumption.}
\end{figure}

\section{Number of Migrations}
Figure 9.2 illustrates the significant reduction in migrations achieved by the Lr-RL algorithm. This reduction is down to the smart selection of VMs to migrate. Better VM Selection means less migration of VMs from both over-utilized and under-utilized hosts. This reduction equated to a total of 283,735 less migrations over the 30 days. This splits into a saving of 60,872 on migrations from over-utilized hosts and 222,863 from under-utilized hosts. This breaks down to a daily average saving of 9457.8 total migrations, 2029 over-utilized host migrations and 7428.8 under-utilized host migrations. This is an average daily saving of 107.9\% in total migrations, 155.37\% in over-utilized migrations and 99.6\% in under-utilized migrations. The paired t test concludes that there is a significant statistical difference in total migrations, returning a two-tailed P value of less than 0.0001 with a 95\% confidence interval (-10128.53, -8787.13).

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.5\textheight]{Figures/Lr_RL-Vs_Lr_Mmt_Total_Migrations.PNG}
\caption{Lr-RL Vs. Lr-Mmt - Total Migration.}
\end{figure}

\section{SLAVs}
As displayed in figure 9.3 the Lr-RL slightly outperforms the Lr-Mmt algorithm with regards to SLAVs. The total and daily average savings on SLAVs is 7.755e-05 and 2.585e-06 respectively, which appears small but in fact equates to a 5.6\% average daily saving. The paired t test concludes that there is no statistical difference in SLAVs, returning a two-tailed P value of less than 0.0018 with a 95\% confidence interval (-0.00000412, -0.00000104).

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.5\textheight]{Figures/Lr_RL-Vs_Lr_Mmt_SLAV.PNG}
\caption{Lr-RL Vs. Lr-Mmt - SLAV.}
\end{figure}

\section{ESV}
The ESV is the most important metric for evaluating the performance of the VM Selection algorithm as it encapsulates both the energy consumption and SLAVs. Figure 9.4 illustrates the difference in ESV between both algorithms across the 30 day workload. The Lr-RL algorithm significantly outperforms the Lr-Mmt algorithm across the vast majority of workloads. This results in a total ESV saving of 0.027 and a daily average saving of 0.0009, which is an average daily improvement of 16.7\%. The paired t test concludes that there is a statistical difference in ESV, returning a two-tailed P value of less than 0.0001 with a 95\% confidence interval (-0.00112580042, -0.00067815312).

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.5\textheight]{Figures/Lr_RL-Vs_Lr_Mmt_ESV.PNG}
\caption{Lr-RL Vs. Lr-Mmt - ESV.}
\end{figure}

\section{Conclusion}
This chapter has clearly demonstrated the success of utilizing the Lr-RL algorithm for VM selection as it significantly improves a state-of-the-art approach in this domain. To further reinforce this point figure 9.5 includes the percentage difference in energy consumption, SLAVs and ESV across each day of the 30 day workload. The final research question of this thesis was to understand whether a RL approach to VM Selection could outperform the state-of-the-art VM Selection algorithm Lr-Mmt. This figure coupled with the analysis from the previous sections, clearly show the effectiveness of the RL approach. With the ESV being the most influential metric for analysis it is important to again note that the Lr-RL algorithm improves the daily average ESV value by 16.7\% when compared to that of the Lr-Mmt algorithm. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=.5\textheight]{Figures/Lr_RL-Vs_Lr_Mmt_Percentage_Improvement.PNG}
\caption{Lr-RL Vs. Lr-Mmt - Percentage Improvement.}
\end{figure}

\chapter{Conclusion}
The demand for Cloud Computing has had a meteoric rise, leading to a huge number of new data centers being constructed all over the world. With the world dealing with the seismic issue that is global warming, now more than ever it has become critical to lower the energy consumed. This rise of Cloud Computing increases the energy consumption of resources that are harmful towards the environment. Data centers consumed 11\% of the Irish energy bill in 2020 [2]. This creates a need for technologies that enable Cloud Computing environments to run more in a more environmentally friendly manner. Any technology that successfully reduces energy consumption, must also maintain a level of service to ensure customer satisfaction. The research carried out in this thesis aimed to evaluate a Reinforcement Learning approach to VM Selection, first proposed by Duggan et al [62]. The aim was to confirm the findings of Duggan et al., proving that Reinforcement Learning is an effective approach to VM Selection. In doing so this research answered the four following questions that were posed in the Introduction chapter of this thesis:

\begin{enumerate}
\item When utilizing the Q-Learning algorithm for VM Selection, what parameters of Alpha and Gamma facilitate the best performance?
\item When utilizing the SARSA algorithm for VM Selection, what parameters of Alpha and Gamma facilitate the best performance?
\item When utilizing a Reinforcement Learning Approach to VM Selection, is Q-Learning or SARSA the better performing algorithm?
\item How does the performance of Reinforcement Learning approach presented compare to Lr-Mmt, a state-of-the-art VM Selection policy?
\end{enumerate}

Experiments carried out in Chapter 7 aimed to answer the first two research questions by analysing a series of hyper-parameters, Alpha and Gamma, to find the best performing parameters for each algorithm. These experiments concluded that for Q-Learning a low Gamma value was critical and an increase would cause a large decrease in performance. A change in Alpha had much less of an effect on performance, although a lower value was optimal. An Alpha and Gamma value of 0.2 were found to be optimal for Q-Learning. With the SARSA algorithm the experiment concluded that the hyper-parameters were both not critical for performance and that the algorithm performed in a similar manner regardless of the values of each. However, an Alpha value of 1.0 and a Gamma value of 0.4 consistently gave a slight performance increase. 

Chapter 8 carried out an experiment with the goal of answering the third research question posed in the Introduction. This involved a deep analysis of the best performing configurations of both the Q-Learning and SARSA algorithm. The aim was to determine which algorithm resulted in better performance for the task of VM Selection. The experiment concluded that SARSA was the better performing. The SARSA algorithm combined with either the $\epsilon$-greedy or Softmax action selection policy outperformed both the $\epsilon$-greedy and Softmax configurations of the Q-Learning algorithm. The performance of SARSA is also significantly less affected by the chosen hyper-parameter values of Alpha and Gamma. 

Chapter 9 conducted an experiment to answer the fourth and final research question. The aim here was to compare the performance of the RL VM Selection algorithm (Lr-RL), to that of the state-of-the-art algorithm Lr-Mmt. The goal here was to identify if the proposed algorithm showed any improvements over the established Lr-Mmt approach. The experiment concluded that the Lr-RL algorithm performed significantly better across a 30 day workload. The algorithm resulted in an average daily saving of 10.37\% on energy, 107.9\% on migrations, 5.6\% on SLAVs and 16.6\% on ESV. These results are in line with the results achieved by Duggan et al., proving that RL is an effective approach to VM Selection that warrants further research [62]. 

\section{Future Work}
While this research proved that RL is an effective approach to VM Selection, RL can also be successfully utilized in other areas such as VM Placement. As was concluded in research carried out by Shaw et al. [63]. Therefore, future work could be carried out to evaluating the feasibility of combining both of these approaches. This would consist of an RL approach to both VM Selection and VM Placement. With both approaches achieving impressive results, together the results could go far beyond the performance of either approach in isolation. 

\begin{thebibliography}{9}

\bibitem{ref1} 
Shi, Y., Jiang, X. & Ye, K. An energy-efficient scheme for cloud resource provisioning based on cloudsim in Cluster Computing (CLUSTER), 2011 IEEE International Conference, (2011), 595–599

\bibitem{ref2} 
Report on Ireland's Data Hosting Industry 2017. https://www.bitpower.ie/index.php/news/20-bitpower-launches-report-on-ireland-s-data-hosting-industry

\bibitem{ref2} 
Brown, R., Masanet, E., Nordman, B., Tschudi, B., Shehabi, A., Stanley, J., Koomey, J., Sartor, D. & Chan, P. Report to congress on server and data center energy efficiency: Public law 109-431. Lawrence Berkeley National Laboratory (2008).

\bibitem{ref2} 
Eirgridgroup.com. 2021. Consultation on Data Centre Connection Offer Process & Policy. [online] Available at: <https://www.eirgridgroup.com/site-files/library/EirGrid/Data-Centre-Connection-Offer-Process-and-Policy-Consultation-Paper.pdf> [Accessed 15 July 2021].

\bibitem{ref3} 
P. Delforge, "America's Data Centers Are Wasting Huge Amounts of Energy," National Resources Defense Council, Issue Brief 14-08-A, August 2014.

\bibitem{ref2} 
Domain 3: Security Engineering (Engineering and Management of Security) Eric Conrad, ... Joshua Feldman, in CISSP Study Guide (Third Edition), 2016

\bibitem{ref2} 
Foster, I. T., Zhao, Y., Raicu, I. & Lu, S. Cloud Computing and Grid Computing 360-Degree Compared. CoRR (2009).

\bibitem{ref2} 
Buyya, R., Yeo, C. S., Venugopal, S., Broberg, J. & Brandic, I. Cloud Computing and Emerging IT Platforms: Vision, Hype, and Reality for Delivering

\bibitem{ref2} 
Computing As the 5th Utility. Future Gener. Comput. Syst. 599–616 (2009).

\bibitem{ref2} 
Vaquero, Luis & Rodero-Merino, Luis & Caceres, Juan & Lindner, Maik. (2009). A Break in the Clouds: Towards a Cloud Definition. Computer Communication Review. 39. 50-55. 10.1145/1496091.1496100. 

\bibitem{ref2} 
Rajkumar Buyya, Chee Shin Yeo, and Srikumar Venugopal. Market-oriented cloud computing: Vision, hype, and reality for delivering it services as computing utilities. CoRR, (abs/0808.3558), 2008.

\bibitem{ref2} 
Mell, P. M. & Grance, T. SP 800-145. The NIST Definition of Cloud Computing tech. rep. (Gaithersburg, MD, United States, 2011)

\bibitem{ref2} 
Reza, S., Adel, A. & Justice, O. M., 2013. Cloud Computing From SMEs Perspective: A Survey Based Investigation. Journal of Information Technology Management, 24(1), pp. 1-12

\bibitem{ref2} 
Zhang, Q., Cheng, L. & Boutaba, R. Cloud computing: state-of-the-art and research challenges. Journal of Internet Services and Applications (2010).

\bibitem{ref2} 
Apostu, A. et al., 2013. Study on Advantages and Disadvantges of Cloud Computing - The Advantages of Telemtry Applications in the Cloud. Morioka City, Iwate, Japan, World Scientific and Engineering Academy and Society.

\bibitem{ref2} 
Armbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica, I. & Zaharia, M. A View of Cloud Computing. Commun. ACM (2010).

\bibitem{ref2} 
Gong, C., Liu, J., Zhang, Q., Chen, H. & Gong, Z. The Characteristics of Cloud Computing in Proceedings of the 2010 39th International Conference on Parallel Processing Workshops (IEEE Computer Society, 2010).

\bibitem{ref2} 
Ta-Tao, C., Kazuo, N. & Thoma, T. C., 2015. An Exploratory Study of Expected Business Value of Cloud Computing. Issues in Information Systems, 16(4), pp. 37-47. 

\bibitem{ref2} 
Sean, M. et al., 2011. Cloud Computing - The Business Perspective. Decision Support Systems, 51(1), pp. 176-189. 

\bibitem{ref2} 
Zhang, Q., Cheng, L. & Boutaba, R. Cloud computing: state-of-the-art and research challenges. Journal of Internet Services and Applications (2010).

\bibitem{ref2} 
Youseff, L., Butrico, M. & Silva, D. D. Towards a unified ontology of cloud computing in Proc. of the Grid Computing Environments Workshop (GCE08) (2008).

\bibitem{ref2} 
Kepes, B. Understanding the Cloud Computing Stack: SaaS, PaaS, IaaS. <www.rackspace.com> (2013).

\bibitem{ref2} 
Brohi, Sarfraz & Bamiah, Mervat. (2013). Challenges and Benefits for Adopting the Paradigm of Cloud Computing. 

\bibitem{ref2} 
K. Bakshi, "Secure hybrid cloud computing: Approaches and use cases," 2014 IEEE Aerospace Conference, 2014, pp. 1-8, doi: 10.1109/AERO.2014.6836198.

\bibitem{ref2} 
Foerster, Theodor & Baranski, Bastian & Schäffer, Bastian & Lange, Kristof. (2021). Geoprocessing in Hybrid Clouds. 

\bibitem{ref2} 
Nicanfar, Hasen & Liu, Qiang & Talebifard, Peyman & Cai, Wei. (2013). Community Cloud: Concept, Model, Attacks and Solution. Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom. 2. 126-131. 10.1109/CloudCom.2013.163. 

\bibitem{ref2} 
Abdul, S. What is Cloud? <www.psau.edu.sa> (2012).

\bibitem{ref2} 
Alashhab, Z., Anbar, M., Singh, M., Leau, Y., Al-Sai, Z. and Abu Alhayja’a, S., 2021. Impact of coronavirus pandemic crisis on technologies and cloud computing applications. Journal of Electronic Science and Technology, 19(1), p.100059.

\bibitem{ref2} 
Independent, Coronavirus: vodafone, O2 and other networks struggle amid huge surge in traffic, [Online]. Available https://www.independent.co.uk/life-style/gadgetsand-tech/news/coronavirus-vodafone-o2-talktalk-down-outage-dataa9411286.html (April 2020)

\bibitem{ref2} 
Irish Examiner, 2021. Data centres: Are they unwanted but necessary guests in our landscape?. [online] Available at: <https://www.irishexaminer.com/news/spotlight/arid-40252148.html> [Accessed 11 July 2021].

\bibitem{ref2} 
IEA (2020), Data Centres and Data Transmission Networks, IEA, Paris https://www.iea.org/reports/data-centres-and-data-transmission-networks.

\bibitem{ref2} 
Masanet, E., Shehabi, A., Lei, N., Smith, S. and Koomey, J., 2020. Recalibrating global data center energy-use estimates. Science, 367(6481), pp.984-986.

\bibitem{ref2} 
Whitney, J. & Delforge, P. Data Center Efficiency Assessment tech. rep. (Natural Resources Defense Council, 2014).

\bibitem{ref2} 
Rong, Huigui & Zhang, Haomin & Xiao, Sheng & Li, Canbing & Hu, Chunhua. (2016). Optimizing energy consumption for data centers. Renewable and Sustainable Energy Reviews. 58. 674-691. 10.1016/j.rser.2015.12.283. 

\bibitem{ref2} 
Uptime Institute, 2013 Uptime Institute Data Center Industry Survey.

\bibitem{ref2} 
Sharma, Madhu & Arunachalam, Kartik & Sharma, Dharani. (2015). Analyzing the Data Center Efficiency by Using PUE to Make Data Centers More Energy Efficient by Reducing the Electrical Consumption and Exploring New Strategies. Procedia Computer Science. 48. 142-148. 10.1016/j.procs.2015.04.163. 

\bibitem{ref2} 
Xing, Y. and Zhan, Y., 2012. Virtualization and Cloud Computing. Lecture Notes in Electrical Engineering, pp.305-312.

\bibitem{ref2} 
Journal of Information Technology & Software Engineering, 2014. Virtualization in Cloud Computing. 04(02).

\bibitem{ref2} 
Minhas, Umar Farooq. (2007). A Performance Evaluation of Database Systems on Virtual Machines. 

\bibitem{ref2} 
Géron, A., 2020. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. Beijing: O'Reilly.

\bibitem{ref2} 
Russell, S. and Norvig, P., n.d. Artificial Intelligence A Modern Approach. 

\bibitem{ref2} 
Duggan, Martin & Flesk, Kieran & Duggan, Jim & Howley, Enda & Barrett, Enda. (2016). A Reinforcement Learning Approach for Dynamic Selection of Virtual Machines in Cloud Data Centres.

\bibitem{ref2} 
<https://plato.stanford.edu/archives/win2019/entries/prisoner-dilemma/> 

\bibitem{ref2} 
Wooldridge, M. An Introduction to MultiAgent Systems 2nd (Wiley Publishing, 2009).

\bibitem{ref2} 
Dorri, Ali & Kanhere, Salil & Jurdak, Raja. (2018). Multi-Agent Systems: A survey. IEEE Access. 6. 1-1. 10.1109/ACCESS.2018.2831228. 

\bibitem{ref2} 
Parasumanna Gokulan, Balaji & Srinivasan, D.. (2010). An Introduction to Multi-Agent Systems. 10.1007/978-3-642-14435-6-1. 

\bibitem{ref2} 
Doran, James & Franklin, Stan & Jennings, N. & Norman, Timothy. (2000). On Cooperation in Multi-Agent Systems. The Knowledge Engineering Review. 12. 10.1017/S0269888997003111. 

\bibitem{ref2} 
Alshabi, W. & Ramaswamy, Srini & Itmi, Mhamed & Abdulrab, H.. (2007). Coordination, Cooperation and Conflict Resolution in Multi-Agent Systems. 10.1007/978-1-4020-6268-1-87. 

\bibitem{ref2} 
Kuhn, Steven, "Prisoner’s Dilemma", The Stanford Encyclopedia of Philosophy (Winter 2019 Edition), Edward N. Zalta (ed.)

\bibitem{ref2} 
Enda Barrett, Enda Howley, and Jim Duggan. Applying reinforcement learning towards automating resource allocation and application scalability in the cloud. Concurrency and Computation: Practice and Experience, 25(12):1656–1674, 2013.

\bibitem{ref2} 
Rummery, G. & Niranjan, Mahesan. (1994). On-Line Q-Learning Using Connectionist Systems. Technical Report CUED/F-INFENG/TR 166. 

\bibitem{ref2} 
Lee, Y. and Zomaya, A., 2010. Energy efficient utilization of resources in cloud computing systems. The Journal of Supercomputing, 60(2), pp.268-280.

\bibitem{ref2} 
Srikantaiah, Shekhar et al. “Energy aware consolidation for cloud computing.” CLUSTER 2008 (2008).

\bibitem{ref2} 
Beloglazov, A. and Buyya, R., 2011. Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in Cloud data centers. Concurrency and Computation: Practice and Experience, 24(13), pp.1397-1420.

\bibitem{ref2} 
Cardosa, M., Korupolu, M. and Singh, A., 2009. Shares and utilities based power consolidation in virtualized server environments. 2009 IFIP/IEEE International Symposium on Integrated Network Management,.

\bibitem{ref2} 
Kusic, D., Kephart, J., Hanson, J., Kandasamy, N. and Jiang, G., 2008. Power and Performance Management of Virtualized Computing Environments Via Lookahead Control. 2008 International Conference on Autonomic Computing,.

\bibitem{ref2} 
Portaluri, Giuseppe & Giordano, S. & Kliazovich, Dzmitry & Dorronsoro, Bernabe. (2014). A Power Efficient Genetic Algorithm for Resource Allocation in Cloud Computing Data Centers. 2014 IEEE 3rd International Conference on Cloud Networking, CloudNet 2014. 10.1109/CloudNet.2014.6968969. 

\bibitem{ref2} 
Dashti, Seyed & Rahmani, Amir. (2015). Dynamic VMs placement for energy efficiency by PSO in cloud computing. Journal of Experimental & Theoretical Artificial Intelligence. 28. 1-16. 10.1080/0952813X.2015.1020519. 

\bibitem{ref2} 
Wei, Guiyi & Vasilakos, Athanasios & Zheng, Yao & Xiong, Naixue. (2010). A Game-Theoretic Method of Fair Resource Allocation for Cloud Computing Services. The Journal of Supercomputing. 54. 252-269. 10.1007/s11227-009-0318-1. 

\bibitem{ref2} 
Berral, Josep & Goiri, Iñigo & Nou, Ramon & Julià, Ferran & Guitart, Jordi & Gavaldà, Ricard & Torres, Jordi. (2010). Towards energy-aware scheduling in data centers using machine learning. Proceedings of the e-Energy 2010 - 1st Int'l Conf. on Energy-Efficient Computing and Networking. 215-224. 10.1145/1791314.1791349. 

\bibitem{ref2} 
Das, Rajarshi & Kephart, Jeffrey & Lefurgy, Charles & Tesauro, Gerald & Levine, David & Chan, Hoi. (2008). Autonomic multi-agent management of power and performance in data centers. Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems: industrial track. 3. 107-114. 

\bibitem{ref2} 
Duggan, Martin & Flesk, Kieran & Duggan, Jim & Howley, Enda & Barrett, Enda. (2016). A Reinforcement Learning Approach for Dynamic Selection of Virtual Machines in Cloud Data Centres. 10.1109/INTECH.2016.7845053. 

\bibitem{ref2} 
Shaw, Rachael & Howley, Enda & Barrett, Enda. (2017). An Advanced Reinforcement Learning Approach for Energy-Aware Virtual Machine Consolidation in Cloud Data Centers. 10.23919/ICITST.2017.8356347. 

\bibitem{ref2} 
Calheiros, Rodrigo & Ranjan, R. & Beloglazov, Anton & De Rose, Cesar & Buyya, Rajkumar. (2011). CloudSim: A Toolkit for Modeling and Simulation of Cloud Computing Environments and Evaluation of Resource Provisioning Algorithms. Software Practice and Experience. 41. 23-50. 10.1002/spe.995. 

\end{document}
